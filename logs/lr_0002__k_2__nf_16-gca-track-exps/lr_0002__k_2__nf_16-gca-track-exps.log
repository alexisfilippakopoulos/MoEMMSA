2026-01-18 12:14:09,894 - MMSA [INFO] - ======================================== Program Start ========================================
2026-01-18 12:14:09,952 - MMSA [INFO] - Running with args:
2026-01-18 12:14:09,952 - MMSA [INFO] - {'model_name': 'msalm', 'dataset_name': 'sims', 'featurePath': '/leonardo_work/EUHPC_A04_051/alexfil/MSA-Datasets/CH-SIMS/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 10, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 8, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'use_ulgm': False, 'update_labels_patience': 1, 'H': 3.0, 'del_model': True, 'max_token_len': 39, 'pad_token': '<|endoftext|>', 'lm': '/leonardo_work/EUHPC_A04_051/alexfil/gpt2-chinese-cluecorpussmall', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.0, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 3, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 16, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [5, 6, 7, 8, 9, 10, 11], 'top_k': 2, 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.3, 'p_apply': 0.25, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': True, 'path_to_pretrained': '/leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth', 'feature_dims': [768, 33, 709], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 55, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': True, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.1, 'weight_decay_av': 0.1, 'learning_rate_av': 0.0002, 'learning_rate_mmgpt': 0.0002, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}
2026-01-18 12:14:09,952 - MMSA [INFO] - Seeds: [1990]
2026-01-18 12:14:09,953 - MMSA [INFO] - ------------------------------ Running with seed 1990 [1/1] ------------------------------
2026-01-18 12:15:29,889 - MMSA [INFO] - The model has 173499052 trainable parameters
2026-01-18 12:15:40,015 - MMSA [INFO] - TRAIN-(msalm) [1/1/1] >> loss: 0.6159  Mult_acc_2: 0.6484  Mult_acc_3: 0.4678  Mult_acc_5: 0.2054  F1_score: 0.6012  MAE: 0.6138  Corr: 0.0774  clm loss: 7.6555 total loss: 10.7579 bn loss: 0.8737 av loss: 0.6627 text loss: 0.9502
2026-01-18 12:15:41,948 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.6754  Mult_acc_3: 0.4803  Mult_acc_5: 0.2061  F1_score: 0.6530  MAE: 0.5781  Corr: 0.2302  Loss: 0.5754 
2026-01-18 12:15:51,305 - MMSA [INFO] - TRAIN-(msalm) [1/2/1] >> loss: 0.5582  Mult_acc_2: 0.6915  Mult_acc_3: 0.5300  Mult_acc_5: 0.2573  F1_score: 0.6636  MAE: 0.5564  Corr: 0.3052  clm loss: 6.3801 total loss: 9.0234 bn loss: 0.683 av loss: 0.614 text loss: 0.7882
2026-01-18 12:15:53,012 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7390  Mult_acc_3: 0.5658  Mult_acc_5: 0.2697  F1_score: 0.7279  MAE: 0.5152  Corr: 0.4522  Loss: 0.5217 
2026-01-18 12:16:02,289 - MMSA [INFO] - TRAIN-(msalm) [1/3/1] >> loss: 0.5193  Mult_acc_2: 0.7156  Mult_acc_3: 0.5651  Mult_acc_5: 0.2785  F1_score: 0.7054  MAE: 0.5082  Corr: 0.4549  clm loss: 5.3667 total loss: 7.7167 bn loss: 0.5831 av loss: 0.5806 text loss: 0.667
2026-01-18 12:16:04,011 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7039  Mult_acc_3: 0.5702  Mult_acc_5: 0.2829  F1_score: 0.7093  MAE: 0.5098  Corr: 0.4704  Loss: 0.5207 
2026-01-18 12:16:13,291 - MMSA [INFO] - TRAIN-(msalm) [1/4/1] >> loss: 0.5069  Mult_acc_2: 0.7507  Mult_acc_3: 0.6060  Mult_acc_5: 0.3202  F1_score: 0.7478  MAE: 0.4881  Corr: 0.5037  clm loss: 5.0427 total loss: 7.3325 bn loss: 0.5573 av loss: 0.5661 text loss: 0.6596
2026-01-18 12:16:15,032 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.6996  Mult_acc_3: 0.5680  Mult_acc_5: 0.2851  F1_score: 0.7062  MAE: 0.5105  Corr: 0.4735  Loss: 0.5282 
2026-01-18 12:16:23,266 - MMSA [INFO] - TRAIN-(msalm) [2/5/1] >> loss: 0.4921  Mult_acc_2: 0.7602  Mult_acc_3: 0.6301  Mult_acc_5: 0.3582  F1_score: 0.7599  MAE: 0.4671  Corr: 0.5331  clm loss: 4.6041 total loss: 6.8296 bn loss: 0.5513 av loss: 0.5577 text loss: 0.6245
2026-01-18 12:16:25,081 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.6930  Mult_acc_3: 0.5746  Mult_acc_5: 0.3136  F1_score: 0.7032  MAE: 0.5257  Corr: 0.4745  Loss: 0.5632 
2026-01-18 12:16:33,462 - MMSA [INFO] - TRAIN-(msalm) [3/6/1] >> loss: 0.4827  Mult_acc_2: 0.7654  Mult_acc_3: 0.6389  Mult_acc_5: 0.3618  F1_score: 0.7634  MAE: 0.4587  Corr: 0.5540  clm loss: 4.0796 total loss: 6.2391 bn loss: 0.5469 av loss: 0.5335 text loss: 0.5964
2026-01-18 12:16:35,176 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.6798  Mult_acc_3: 0.5592  Mult_acc_5: 0.2851  F1_score: 0.6917  MAE: 0.5191  Corr: 0.4955  Loss: 0.5364 
2026-01-18 12:16:43,410 - MMSA [INFO] - TRAIN-(msalm) [4/7/1] >> loss: 0.4691  Mult_acc_2: 0.7719  Mult_acc_3: 0.6440  Mult_acc_5: 0.3553  F1_score: 0.7686  MAE: 0.4579  Corr: 0.5588  clm loss: 3.993 total loss: 6.1123 bn loss: 0.5275 av loss: 0.5258 text loss: 0.597
2026-01-18 12:16:45,124 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7149  Mult_acc_3: 0.5811  Mult_acc_5: 0.3224  F1_score: 0.7206  MAE: 0.4941  Corr: 0.5081  Loss: 0.5063 
2026-01-18 12:16:54,419 - MMSA [INFO] - TRAIN-(msalm) [1/8/1] >> loss: 0.4767  Mult_acc_2: 0.7507  Mult_acc_3: 0.6111  Mult_acc_5: 0.3304  F1_score: 0.7476  MAE: 0.4660  Corr: 0.5441  clm loss: 3.8493 total loss: 5.9612 bn loss: 0.5294 av loss: 0.5149 text loss: 0.5909
2026-01-18 12:16:56,135 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7566  Mult_acc_3: 0.6360  Mult_acc_5: 0.3794  F1_score: 0.7548  MAE: 0.4666  Corr: 0.5396  Loss: 0.4834 
2026-01-18 12:17:05,370 - MMSA [INFO] - TRAIN-(msalm) [1/9/1] >> loss: 0.4626  Mult_acc_2: 0.7507  Mult_acc_3: 0.6440  Mult_acc_5: 0.3458  F1_score: 0.7498  MAE: 0.4518  Corr: 0.5752  clm loss: 3.6048 total loss: 5.674 bn loss: 0.5224 av loss: 0.5007 text loss: 0.5835
2026-01-18 12:17:07,053 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7434  Mult_acc_3: 0.6228  Mult_acc_5: 0.3750  F1_score: 0.7432  MAE: 0.4675  Corr: 0.5409  Loss: 0.4924 
2026-01-18 12:17:15,262 - MMSA [INFO] - TRAIN-(msalm) [2/10/1] >> loss: 0.4521  Mult_acc_2: 0.7624  Mult_acc_3: 0.6462  Mult_acc_5: 0.3779  F1_score: 0.7625  MAE: 0.4381  Corr: 0.5881  clm loss: 3.5462 total loss: 5.566 bn loss: 0.5058 av loss: 0.5002 text loss: 0.5617
2026-01-18 12:17:16,983 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7303  Mult_acc_3: 0.6206  Mult_acc_5: 0.3531  F1_score: 0.7335  MAE: 0.4697  Corr: 0.5396  Loss: 0.5044 
2026-01-18 12:17:25,178 - MMSA [INFO] - TRAIN-(msalm) [3/11/1] >> loss: 0.4431  Mult_acc_2: 0.7610  Mult_acc_3: 0.6506  Mult_acc_5: 0.3721  F1_score: 0.7601  MAE: 0.4312  Corr: 0.6092  clm loss: 3.4292 total loss: 5.4444 bn loss: 0.5158 av loss: 0.4862 text loss: 0.5701
2026-01-18 12:17:26,952 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.6732  Mult_acc_3: 0.5768  Mult_acc_5: 0.3180  F1_score: 0.6858  MAE: 0.4913  Corr: 0.5407  Loss: 0.5379 
2026-01-18 12:17:35,168 - MMSA [INFO] - TRAIN-(msalm) [4/12/1] >> loss: 0.4296  Mult_acc_2: 0.7719  Mult_acc_3: 0.6601  Mult_acc_5: 0.3896  F1_score: 0.7734  MAE: 0.4144  Corr: 0.6245  clm loss: 3.3695 total loss: 5.3327 bn loss: 0.5008 av loss: 0.4756 text loss: 0.5572
2026-01-18 12:17:36,906 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.6886  Mult_acc_3: 0.5833  Mult_acc_5: 0.3289  F1_score: 0.7001  MAE: 0.4782  Corr: 0.5514  Loss: 0.5252 
2026-01-18 12:17:45,110 - MMSA [INFO] - TRAIN-(msalm) [5/13/1] >> loss: 0.4341  Mult_acc_2: 0.7741  Mult_acc_3: 0.6579  Mult_acc_5: 0.3918  F1_score: 0.7752  MAE: 0.4188  Corr: 0.6259  clm loss: 3.2924 total loss: 5.2242 bn loss: 0.4852 av loss: 0.4637 text loss: 0.5488
2026-01-18 12:17:46,821 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.6864  Mult_acc_3: 0.5921  Mult_acc_5: 0.3421  F1_score: 0.6981  MAE: 0.4736  Corr: 0.5561  Loss: 0.5048 
2026-01-18 12:17:55,120 - MMSA [INFO] - TRAIN-(msalm) [6/14/1] >> loss: 0.4222  Mult_acc_2: 0.7675  Mult_acc_3: 0.6623  Mult_acc_5: 0.3955  F1_score: 0.7696  MAE: 0.4086  Corr: 0.6508  clm loss: 3.2769 total loss: 5.2734 bn loss: 0.5403 av loss: 0.4718 text loss: 0.5622
2026-01-18 12:17:56,872 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7193  Mult_acc_3: 0.6250  Mult_acc_5: 0.3816  F1_score: 0.7260  MAE: 0.4426  Corr: 0.5626  Loss: 0.4740 
2026-01-18 12:18:06,158 - MMSA [INFO] - TRAIN-(msalm) [1/15/1] >> loss: 0.424  Mult_acc_2: 0.7719  Mult_acc_3: 0.6564  Mult_acc_5: 0.3794  F1_score: 0.7747  MAE: 0.4103  Corr: 0.6546  clm loss: 3.2759 total loss: 5.1964 bn loss: 0.4996 av loss: 0.4665 text loss: 0.5305
2026-01-18 12:18:07,887 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7083  Mult_acc_3: 0.6447  Mult_acc_5: 0.4013  F1_score: 0.7170  MAE: 0.4457  Corr: 0.5679  Loss: 0.4754 
2026-01-18 12:18:16,070 - MMSA [INFO] - TRAIN-(msalm) [2/16/1] >> loss: 0.4071  Mult_acc_2: 0.7749  Mult_acc_3: 0.6754  Mult_acc_5: 0.4262  F1_score: 0.7777  MAE: 0.3858  Corr: 0.6822  clm loss: 3.2599 total loss: 5.1352 bn loss: 0.478 av loss: 0.4657 text loss: 0.5246
2026-01-18 12:18:17,768 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7193  Mult_acc_3: 0.6382  Mult_acc_5: 0.3925  F1_score: 0.7282  MAE: 0.4441  Corr: 0.5738  Loss: 0.4891 
2026-01-18 12:18:25,931 - MMSA [INFO] - TRAIN-(msalm) [3/17/1] >> loss: 0.4103  Mult_acc_2: 0.7858  Mult_acc_3: 0.6652  Mult_acc_5: 0.3947  F1_score: 0.7872  MAE: 0.3964  Corr: 0.6742  clm loss: 3.2264 total loss: 5.114 bn loss: 0.4911 av loss: 0.4636 text loss: 0.5226
2026-01-18 12:18:27,682 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.6776  Mult_acc_3: 0.6184  Mult_acc_5: 0.3575  F1_score: 0.6904  MAE: 0.4529  Corr: 0.5990  Loss: 0.4795 
2026-01-18 12:18:35,910 - MMSA [INFO] - TRAIN-(msalm) [4/18/1] >> loss: 0.4065  Mult_acc_2: 0.7844  Mult_acc_3: 0.6645  Mult_acc_5: 0.3838  F1_score: 0.7851  MAE: 0.3941  Corr: 0.6848  clm loss: 3.2333 total loss: 5.0887 bn loss: 0.4759 av loss: 0.4644 text loss: 0.5086
2026-01-18 12:18:37,604 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7456  Mult_acc_3: 0.6469  Mult_acc_5: 0.4035  F1_score: 0.7525  MAE: 0.4212  Corr: 0.6061  Loss: 0.4517 
2026-01-18 12:18:46,834 - MMSA [INFO] - TRAIN-(msalm) [1/19/1] >> loss: 0.3999  Mult_acc_2: 0.7931  Mult_acc_3: 0.6849  Mult_acc_5: 0.4086  F1_score: 0.7943  MAE: 0.3855  Corr: 0.6894  clm loss: 3.2555 total loss: 5.078 bn loss: 0.4629 av loss: 0.4549 text loss: 0.5048
2026-01-18 12:18:48,551 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7303  Mult_acc_3: 0.6447  Mult_acc_5: 0.4079  F1_score: 0.7387  MAE: 0.4234  Corr: 0.6075  Loss: 0.4507 
2026-01-18 12:18:57,786 - MMSA [INFO] - TRAIN-(msalm) [1/20/1] >> loss: 0.402  Mult_acc_2: 0.7931  Mult_acc_3: 0.6762  Mult_acc_5: 0.4115  F1_score: 0.7964  MAE: 0.3883  Corr: 0.6878  clm loss: 3.2058 total loss: 5.092 bn loss: 0.5183 av loss: 0.467 text loss: 0.4987
2026-01-18 12:18:59,528 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7675  Mult_acc_3: 0.6491  Mult_acc_5: 0.4035  F1_score: 0.7697  MAE: 0.4050  Corr: 0.6288  Loss: 0.4136 
2026-01-18 12:19:08,722 - MMSA [INFO] - TRAIN-(msalm) [1/21/1] >> loss: 0.3843  Mult_acc_2: 0.8041  Mult_acc_3: 0.6857  Mult_acc_5: 0.4196  F1_score: 0.8067  MAE: 0.3697  Corr: 0.7184  clm loss: 3.1588 total loss: 4.9844 bn loss: 0.4741 av loss: 0.4616 text loss: 0.5056
2026-01-18 12:19:10,412 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7325  Mult_acc_3: 0.6535  Mult_acc_5: 0.3816  F1_score: 0.7410  MAE: 0.4125  Corr: 0.6341  Loss: 0.4305 
2026-01-18 12:20:47,895 - MMSA [INFO] - ======================================== Program Start ========================================
2026-01-18 12:20:47,966 - MMSA [INFO] - Running with args:
2026-01-18 12:20:47,966 - MMSA [INFO] - {'model_name': 'msalm', 'dataset_name': 'sims', 'featurePath': '/leonardo_work/EUHPC_A04_051/alexfil/MSA-Datasets/CH-SIMS/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 10, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 1, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'use_ulgm': False, 'update_labels_patience': 1, 'H': 3.0, 'del_model': True, 'max_token_len': 39, 'pad_token': '<|endoftext|>', 'lm': '/leonardo_work/EUHPC_A04_051/alexfil/gpt2-chinese-cluecorpussmall', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.0, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 3, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 16, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [5, 6, 7, 8, 9, 10, 11], 'top_k': 2, 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.3, 'p_apply': 0.25, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': True, 'path_to_pretrained': '/leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth', 'feature_dims': [768, 33, 709], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 55, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': True, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.1, 'weight_decay_av': 0.1, 'learning_rate_av': 0.0002, 'learning_rate_mmgpt': 0.0002, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}
2026-01-18 12:20:47,966 - MMSA [INFO] - Seeds: [1990]
2026-01-18 12:20:47,966 - MMSA [INFO] - ------------------------------ Running with seed 1990 [1/1] ------------------------------
2026-01-18 12:22:04,916 - MMSA [INFO] - The model has 173499052 trainable parameters
2026-01-18 12:22:14,964 - MMSA [INFO] - TRAIN-(msalm) [1/1/1] >> loss: 0.5633  Mult_acc_2: 0.6923  Mult_acc_3: 0.5227  Mult_acc_5: 0.2573  F1_score: 0.6642  MAE: 0.5582  Corr: 0.2897  clm loss: 6.5772 total loss: 9.2783 bn loss: 0.726 av loss: 0.6165 text loss: 0.7953
2026-01-18 12:22:16,918 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7544  Mult_acc_3: 0.6053  Mult_acc_5: 0.3399  F1_score: 0.7435  MAE: 0.5243  Corr: 0.4676  Loss: 0.5684 
2026-01-18 12:22:26,636 - MMSA [INFO] - TRAIN-(msalm) [1/2/1] >> loss: 0.4991  Mult_acc_2: 0.7485  Mult_acc_3: 0.6096  Mult_acc_5: 0.3341  F1_score: 0.7462  MAE: 0.4844  Corr: 0.4972  clm loss: 3.8354 total loss: 6.029 bn loss: 0.5567 av loss: 0.5028 text loss: 0.635
2026-01-18 12:22:28,423 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7346  Mult_acc_3: 0.6162  Mult_acc_5: 0.3553  F1_score: 0.7333  MAE: 0.4701  Corr: 0.5129  Loss: 0.5064 
2026-01-18 12:22:37,694 - MMSA [INFO] - TRAIN-(msalm) [1/3/1] >> loss: 0.4557  Mult_acc_2: 0.7566  Mult_acc_3: 0.6330  Mult_acc_5: 0.3428  F1_score: 0.7562  MAE: 0.4401  Corr: 0.5944  clm loss: 3.3292 total loss: 5.3282 bn loss: 0.5073 av loss: 0.4697 text loss: 0.5663
2026-01-18 12:22:39,353 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.6711  Mult_acc_3: 0.5768  Mult_acc_5: 0.3311  F1_score: 0.6840  MAE: 0.4858  Corr: 0.5636  Loss: 0.5081 
2026-01-18 12:22:47,584 - MMSA [INFO] - TRAIN-(msalm) [2/4/1] >> loss: 0.4256  Mult_acc_2: 0.7727  Mult_acc_3: 0.6535  Mult_acc_5: 0.3684  F1_score: 0.7733  MAE: 0.4167  Corr: 0.6411  clm loss: 3.2793 total loss: 5.1802 bn loss: 0.475 av loss: 0.4617 text loss: 0.5386
2026-01-18 12:22:49,612 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7412  Mult_acc_3: 0.6294  Mult_acc_5: 0.3531  F1_score: 0.7449  MAE: 0.4397  Corr: 0.5770  Loss: 0.4596 
2026-01-18 12:22:59,140 - MMSA [INFO] - TRAIN-(msalm) [1/5/1] >> loss: 0.4503  Mult_acc_2: 0.7705  Mult_acc_3: 0.6477  Mult_acc_5: 0.3633  F1_score: 0.7721  MAE: 0.4328  Corr: 0.6093  clm loss: 3.2208 total loss: 5.2528 bn loss: 0.5258 av loss: 0.4959 text loss: 0.56
2026-01-18 12:23:00,904 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7807  Mult_acc_3: 0.6184  Mult_acc_5: 0.3969  F1_score: 0.7547  MAE: 0.4885  Corr: 0.5927  Loss: 0.5978 
2026-01-18 12:23:09,174 - MMSA [INFO] - TRAIN-(msalm) [2/6/1] >> loss: 0.3927  Mult_acc_2: 0.7902  Mult_acc_3: 0.6813  Mult_acc_5: 0.4042  F1_score: 0.7929  MAE: 0.3738  Corr: 0.7109  clm loss: 3.1209 total loss: 4.9648 bn loss: 0.4728 av loss: 0.4719 text loss: 0.5065
2026-01-18 12:23:10,967 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7741  Mult_acc_3: 0.6711  Mult_acc_5: 0.4276  F1_score: 0.7775  MAE: 0.4001  Corr: 0.6488  Loss: 0.4323 
2026-01-18 12:23:20,288 - MMSA [INFO] - TRAIN-(msalm) [1/7/1] >> loss: 0.3832  Mult_acc_2: 0.8070  Mult_acc_3: 0.6915  Mult_acc_5: 0.4247  F1_score: 0.8097  MAE: 0.3658  Corr: 0.7213  clm loss: 3.1155 total loss: 4.8743 bn loss: 0.4284 av loss: 0.467 text loss: 0.4803
2026-01-18 12:23:22,015 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7237  Mult_acc_3: 0.6447  Mult_acc_5: 0.4057  F1_score: 0.7336  MAE: 0.4127  Corr: 0.6524  Loss: 0.4375 
2026-01-18 12:23:30,235 - MMSA [INFO] - TRAIN-(msalm) [2/8/1] >> loss: 0.3947  Mult_acc_2: 0.7990  Mult_acc_3: 0.6806  Mult_acc_5: 0.3984  F1_score: 0.8017  MAE: 0.3790  Corr: 0.7057  clm loss: 3.0917 total loss: 4.9666 bn loss: 0.4661 av loss: 0.482 text loss: 0.5321
2026-01-18 12:23:31,953 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7500  Mult_acc_3: 0.6557  Mult_acc_5: 0.4123  F1_score: 0.7584  MAE: 0.4186  Corr: 0.6430  Loss: 0.5286 
2026-01-18 12:23:40,146 - MMSA [INFO] - TRAIN-(msalm) [3/9/1] >> loss: 0.3691  Mult_acc_2: 0.8114  Mult_acc_3: 0.7039  Mult_acc_5: 0.4240  F1_score: 0.8141  MAE: 0.3540  Corr: 0.7455  clm loss: 2.9661 total loss: 4.6682 bn loss: 0.4079 av loss: 0.4694 text loss: 0.4556
2026-01-18 12:23:41,858 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.6952  Mult_acc_3: 0.6382  Mult_acc_5: 0.3925  F1_score: 0.7069  MAE: 0.4281  Corr: 0.6710  Loss: 0.4316 
2026-01-18 12:23:51,195 - MMSA [INFO] - TRAIN-(msalm) [1/10/1] >> loss: 0.3382  Mult_acc_2: 0.8304  Mult_acc_3: 0.7230  Mult_acc_5: 0.4554  F1_score: 0.8336  MAE: 0.3256  Corr: 0.7838  clm loss: 2.9369 total loss: 4.5354 bn loss: 0.3708 av loss: 0.4667 text loss: 0.4227
2026-01-18 12:23:52,913 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7390  Mult_acc_3: 0.6623  Mult_acc_5: 0.4189  F1_score: 0.7490  MAE: 0.3986  Corr: 0.6754  Loss: 0.4185 
2026-01-18 12:28:00,593 - MMSA [INFO] - ======================================== Program Start ========================================
2026-01-18 12:28:00,673 - MMSA [INFO] - Running with args:
2026-01-18 12:28:00,673 - MMSA [INFO] - {'model_name': 'msalm', 'dataset_name': 'sims', 'featurePath': '/leonardo_work/EUHPC_A04_051/alexfil/MSA-Datasets/CH-SIMS/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 0, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 1, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'use_ulgm': False, 'update_labels_patience': 1, 'H': 3.0, 'del_model': True, 'max_token_len': 39, 'pad_token': '<|endoftext|>', 'lm': '/leonardo_work/EUHPC_A04_051/alexfil/gpt2-chinese-cluecorpussmall', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.0, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 3, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 16, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [5, 6, 7, 8, 9, 10, 11], 'top_k': 2, 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.3, 'p_apply': 0.25, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': True, 'path_to_pretrained': '/leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth', 'feature_dims': [768, 33, 709], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 55, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': True, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.1, 'weight_decay_av': 0.1, 'learning_rate_av': 0.0002, 'learning_rate_mmgpt': 0.0002, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}
2026-01-18 12:28:00,674 - MMSA [INFO] - Seeds: [1990]
2026-01-18 12:28:00,675 - MMSA [INFO] - ------------------------------ Running with seed 1990 [1/1] ------------------------------
2026-01-18 12:29:21,758 - MMSA [INFO] - The model has 173499052 trainable parameters
2026-01-18 12:29:32,257 - MMSA [INFO] - TRAIN-(msalm) [1/1/1] >> loss: 0.5633  Mult_acc_2: 0.6923  Mult_acc_3: 0.5227  Mult_acc_5: 0.2573  F1_score: 0.6642  MAE: 0.5582  Corr: 0.2897  clm loss: 6.5772 total loss: 9.2783 bn loss: 0.726 av loss: 0.6165 text loss: 0.7953
2026-01-18 12:29:34,231 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7544  Mult_acc_3: 0.6053  Mult_acc_5: 0.3399  F1_score: 0.7435  MAE: 0.5243  Corr: 0.4676  Loss: 0.5684 
2026-01-18 12:29:37,573 - MMSA [INFO] - TEST-(msalm) >>  Mult_acc_2: 0.7724  Mult_acc_3: 0.6280  Mult_acc_5: 0.3611  F1_score: 0.7632  MAE: 0.4835  Corr: 0.5391  Loss: 0.5225 
2026-01-18 12:29:38,843 - MMSA [INFO] - Result for seed 1990: {'Mult_acc_2': 0.7724, 'Mult_acc_3': 0.628, 'Mult_acc_5': 0.3611, 'F1_score': 0.7632, 'MAE': 0.4835, 'Corr': 0.5391, 'Loss': 0.5225, 'seed': 1990}
2026-01-18 12:29:38,864 - MMSA [INFO] - Results saved to MMSA/results/sims/lrs_0002/sparse-mm-token-lvl-router/k_2/nf_16-track-exps/lr_0002__k_2__nf_16-gca-track-exps/normal/sims_avg.csv.
2026-01-18 12:35:10,859 - MMSA [INFO] - ======================================== Program Start ========================================
2026-01-18 12:35:10,947 - MMSA [INFO] - Running with args:
2026-01-18 12:35:10,947 - MMSA [INFO] - {'model_name': 'msalm', 'dataset_name': 'sims', 'featurePath': '/leonardo_work/EUHPC_A04_051/alexfil/MSA-Datasets/CH-SIMS/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 0, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 1, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'use_ulgm': False, 'update_labels_patience': 1, 'H': 3.0, 'del_model': True, 'max_token_len': 39, 'pad_token': '<|endoftext|>', 'lm': '/leonardo_work/EUHPC_A04_051/alexfil/gpt2-chinese-cluecorpussmall', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.0, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 3, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 16, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [5, 6, 7, 8, 9, 10, 11], 'top_k': 2, 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.3, 'p_apply': 0.25, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': True, 'path_to_pretrained': '/leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth', 'feature_dims': [768, 33, 709], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 55, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': True, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.1, 'weight_decay_av': 0.1, 'learning_rate_av': 0.0002, 'learning_rate_mmgpt': 0.0002, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}
2026-01-18 12:35:10,947 - MMSA [INFO] - Seeds: [1990]
2026-01-18 12:35:10,948 - MMSA [INFO] - ------------------------------ Running with seed 1990 [1/1] ------------------------------
2026-01-18 12:36:30,450 - MMSA [INFO] - The model has 173499052 trainable parameters
2026-01-18 12:36:40,644 - MMSA [INFO] - TRAIN-(msalm) [1/1/1] >> loss: 0.5633  Mult_acc_2: 0.6923  Mult_acc_3: 0.5227  Mult_acc_5: 0.2573  F1_score: 0.6642  MAE: 0.5582  Corr: 0.2897  clm loss: 6.5772 total loss: 9.2783 bn loss: 0.726 av loss: 0.6165 text loss: 0.7953
2026-01-18 12:36:42,567 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7544  Mult_acc_3: 0.6053  Mult_acc_5: 0.3399  F1_score: 0.7435  MAE: 0.5243  Corr: 0.4676  Loss: 0.5684 
2026-01-18 12:36:45,688 - MMSA [INFO] - TEST-(msalm) >>  Mult_acc_2: 0.7724  Mult_acc_3: 0.6280  Mult_acc_5: 0.3611  F1_score: 0.7632  MAE: 0.4835  Corr: 0.5391  Loss: 0.5225 
2026-01-18 12:36:46,945 - MMSA [INFO] - Result for seed 1990: {'Mult_acc_2': 0.7724, 'Mult_acc_3': 0.628, 'Mult_acc_5': 0.3611, 'F1_score': 0.7632, 'MAE': 0.4835, 'Corr': 0.5391, 'Loss': 0.5225, 'seed': 1990}
2026-01-18 12:36:46,978 - MMSA [INFO] - Results saved to MMSA/results/sims/lrs_0002/sparse-mm-token-lvl-router/k_2/nf_16-track-exps/lr_0002__k_2__nf_16-gca-track-exps/normal/sims_avg.csv.
2026-01-18 12:54:24,868 - MMSA [INFO] - ======================================== Program Start ========================================
2026-01-18 12:54:24,945 - MMSA [INFO] - Running with args:
2026-01-18 12:54:24,945 - MMSA [INFO] - {'model_name': 'msalm', 'dataset_name': 'sims', 'featurePath': '/leonardo_work/EUHPC_A04_051/alexfil/MSA-Datasets/CH-SIMS/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 0, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 1, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'use_ulgm': False, 'update_labels_patience': 1, 'H': 3.0, 'del_model': True, 'max_token_len': 39, 'pad_token': '<|endoftext|>', 'lm': '/leonardo_work/EUHPC_A04_051/alexfil/gpt2-chinese-cluecorpussmall', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.0, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 3, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 16, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [5, 6, 7, 8, 9, 10, 11], 'top_k': 2, 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.3, 'p_apply': 0.25, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': True, 'path_to_pretrained': '/leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth', 'feature_dims': [768, 33, 709], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 55, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': True, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.1, 'weight_decay_av': 0.1, 'learning_rate_av': 0.0002, 'learning_rate_mmgpt': 0.0002, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}
2026-01-18 12:54:24,945 - MMSA [INFO] - Seeds: [1990]
2026-01-18 12:54:24,946 - MMSA [INFO] - ------------------------------ Running with seed 1990 [1/1] ------------------------------
2026-01-18 12:55:44,270 - MMSA [INFO] - The model has 173499052 trainable parameters
2026-01-18 12:55:54,442 - MMSA [INFO] - TRAIN-(msalm) [1/1/1] >> loss: 0.5633  Mult_acc_2: 0.6923  Mult_acc_3: 0.5227  Mult_acc_5: 0.2573  F1_score: 0.6642  MAE: 0.5582  Corr: 0.2897  clm loss: 6.5772 total loss: 9.2783 bn loss: 0.726 av loss: 0.6165 text loss: 0.7953
2026-01-18 12:55:56,345 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7544  Mult_acc_3: 0.6053  Mult_acc_5: 0.3399  F1_score: 0.7435  MAE: 0.5243  Corr: 0.4676  Loss: 0.5684 
2026-01-18 12:55:59,573 - MMSA [INFO] - TEST-(msalm) >>  Mult_acc_2: 0.7724  Mult_acc_3: 0.6280  Mult_acc_5: 0.3611  F1_score: 0.7632  MAE: 0.4835  Corr: 0.5391  Loss: 0.5225 
2026-01-18 12:56:00,841 - MMSA [INFO] - Result for seed 1990: {'Mult_acc_2': 0.7724, 'Mult_acc_3': 0.628, 'Mult_acc_5': 0.3611, 'F1_score': 0.7632, 'MAE': 0.4835, 'Corr': 0.5391, 'Loss': 0.5225, 'seed': 1990}
2026-01-18 12:56:00,874 - MMSA [INFO] - Results saved to MMSA/results/sims/lrs_0002/sparse-mm-token-lvl-router/k_2/nf_16-track-exps/lr_0002__k_2__nf_16-gca-track-exps/normal/sims_avg.csv.
2026-01-18 14:01:37,355 - MMSA [INFO] - ======================================== Program Start ========================================
2026-01-18 14:01:37,425 - MMSA [INFO] - Running with args:
2026-01-18 14:01:37,425 - MMSA [INFO] - {'model_name': 'msalm', 'dataset_name': 'sims', 'featurePath': '/leonardo_work/EUHPC_A04_051/alexfil/MSA-Datasets/CH-SIMS/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 0, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 1, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'use_ulgm': False, 'update_labels_patience': 1, 'H': 3.0, 'del_model': True, 'max_token_len': 39, 'pad_token': '<|endoftext|>', 'lm': '/leonardo_work/EUHPC_A04_051/alexfil/gpt2-chinese-cluecorpussmall', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.0, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 3, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 16, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [5, 6, 7, 8, 9, 10, 11], 'top_k': 2, 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.3, 'p_apply': 0.25, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': True, 'path_to_pretrained': '/leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth', 'feature_dims': [768, 33, 709], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 55, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': True, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.1, 'weight_decay_av': 0.1, 'learning_rate_av': 0.0002, 'learning_rate_mmgpt': 0.0002, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}
2026-01-18 14:01:37,425 - MMSA [INFO] - Seeds: [1990]
2026-01-18 14:01:37,426 - MMSA [INFO] - ------------------------------ Running with seed 1990 [1/1] ------------------------------
2026-01-18 14:03:06,128 - MMSA [INFO] - The model has 173499052 trainable parameters
2026-01-18 14:03:17,651 - MMSA [INFO] - TRAIN-(msalm) [1/1/1] >> loss: 0.5633  Mult_acc_2: 0.6923  Mult_acc_3: 0.5227  Mult_acc_5: 0.2573  F1_score: 0.6642  MAE: 0.5582  Corr: 0.2897  clm loss: 6.5772 total loss: 9.2783 bn loss: 0.726 av loss: 0.6165 text loss: 0.7953
2026-01-18 14:03:20,006 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7544  Mult_acc_3: 0.6053  Mult_acc_5: 0.3399  F1_score: 0.7435  MAE: 0.5243  Corr: 0.4676  Loss: 0.5684 
2026-01-18 14:03:23,735 - MMSA [INFO] - TEST-(msalm) >>  Mult_acc_2: 0.7724  Mult_acc_3: 0.6280  Mult_acc_5: 0.3611  F1_score: 0.7632  MAE: 0.4835  Corr: 0.5391  Loss: 0.5225 
2026-01-18 14:03:25,078 - MMSA [INFO] - Result for seed 1990: {'Mult_acc_2': 0.7724, 'Mult_acc_3': 0.628, 'Mult_acc_5': 0.3611, 'F1_score': 0.7632, 'MAE': 0.4835, 'Corr': 0.5391, 'Loss': 0.5225, 'seed': 1990}
2026-01-18 14:03:25,119 - MMSA [INFO] - Results saved to MMSA/results/sims/lrs_0002/sparse-mm-token-lvl-router/k_2/nf_16-track-exps/lr_0002__k_2__nf_16-gca-track-exps/normal/sims_avg.csv.
2026-01-18 14:28:43,683 - MMSA [INFO] - ======================================== Program Start ========================================
2026-01-18 14:28:43,775 - MMSA [INFO] - Running with args:
2026-01-18 14:28:43,775 - MMSA [INFO] - {'model_name': 'msalm', 'dataset_name': 'sims', 'featurePath': '/leonardo_work/EUHPC_A04_051/alexfil/MSA-Datasets/CH-SIMS/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 0, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 1, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'use_ulgm': False, 'update_labels_patience': 1, 'H': 3.0, 'del_model': True, 'max_token_len': 39, 'pad_token': '<|endoftext|>', 'lm': '/leonardo_work/EUHPC_A04_051/alexfil/gpt2-chinese-cluecorpussmall', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.0, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 3, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 16, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [5, 6, 7, 8, 9, 10, 11], 'top_k': 2, 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.3, 'p_apply': 0.25, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': True, 'path_to_pretrained': '/leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth', 'feature_dims': [768, 33, 709], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 55, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': True, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.1, 'weight_decay_av': 0.1, 'learning_rate_av': 0.0002, 'learning_rate_mmgpt': 0.0002, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}
2026-01-18 14:28:43,775 - MMSA [INFO] - Seeds: [1990]
2026-01-18 14:28:43,776 - MMSA [INFO] - ------------------------------ Running with seed 1990 [1/1] ------------------------------
2026-01-18 14:30:04,499 - MMSA [INFO] - The model has 173499052 trainable parameters
2026-01-18 14:32:01,696 - MMSA [INFO] - ======================================== Program Start ========================================
2026-01-18 14:32:01,777 - MMSA [INFO] - Running with args:
2026-01-18 14:32:01,777 - MMSA [INFO] - {'model_name': 'msalm', 'dataset_name': 'sims', 'featurePath': '/leonardo_work/EUHPC_A04_051/alexfil/MSA-Datasets/CH-SIMS/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 0, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 1, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'use_ulgm': False, 'update_labels_patience': 1, 'H': 3.0, 'del_model': True, 'max_token_len': 39, 'pad_token': '<|endoftext|>', 'lm': '/leonardo_work/EUHPC_A04_051/alexfil/gpt2-chinese-cluecorpussmall', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.0, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 3, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 16, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [5, 6, 7, 8, 9, 10, 11], 'top_k': 2, 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.3, 'p_apply': 0.25, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': True, 'path_to_pretrained': '/leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth', 'feature_dims': [768, 33, 709], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 55, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': True, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.1, 'weight_decay_av': 0.1, 'learning_rate_av': 0.0002, 'learning_rate_mmgpt': 0.0002, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}
2026-01-18 14:32:01,777 - MMSA [INFO] - Seeds: [1990]
2026-01-18 14:32:01,778 - MMSA [INFO] - ------------------------------ Running with seed 1990 [1/1] ------------------------------
2026-01-18 14:33:21,152 - MMSA [INFO] - The model has 173499052 trainable parameters
2026-01-18 14:33:31,071 - MMSA [INFO] - TRAIN-(msalm) [1/1/1] >> loss: 0.5633  Mult_acc_2: 0.6923  Mult_acc_3: 0.5227  Mult_acc_5: 0.2573  F1_score: 0.6642  MAE: 0.5582  Corr: 0.2897  clm loss: 6.5772 total loss: 9.2783 bn loss: 0.726 av loss: 0.6165 text loss: 0.7953
2026-01-18 14:33:33,034 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7544  Mult_acc_3: 0.6053  Mult_acc_5: 0.3399  F1_score: 0.7435  MAE: 0.5243  Corr: 0.4676  Loss: 0.5684 
2026-01-18 14:33:36,168 - MMSA [INFO] - TEST-(msalm) >>  Mult_acc_2: 0.7724  Mult_acc_3: 0.6280  Mult_acc_5: 0.3611  F1_score: 0.7632  MAE: 0.4835  Corr: 0.5391  Loss: 0.5225 
2026-01-18 14:33:37,429 - MMSA [INFO] - Result for seed 1990: {'Mult_acc_2': 0.7724, 'Mult_acc_3': 0.628, 'Mult_acc_5': 0.3611, 'F1_score': 0.7632, 'MAE': 0.4835, 'Corr': 0.5391, 'Loss': 0.5225, 'seed': 1990}
2026-01-18 14:33:37,462 - MMSA [INFO] - Results saved to MMSA/results/sims/lrs_0002/sparse-mm-token-lvl-router/k_2/nf_16-track-exps/lr_0002__k_2__nf_16-gca-track-exps/normal/sims_avg.csv.
2026-01-18 14:54:20,739 - MMSA [INFO] - ======================================== Program Start ========================================
2026-01-18 14:54:20,796 - MMSA [INFO] - Running with args:
2026-01-18 14:54:20,796 - MMSA [INFO] - {'model_name': 'msalm', 'dataset_name': 'sims', 'featurePath': '/leonardo_work/EUHPC_A04_051/alexfil/MSA-Datasets/CH-SIMS/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 0, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 1, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'use_ulgm': False, 'update_labels_patience': 1, 'H': 3.0, 'del_model': True, 'max_token_len': 39, 'pad_token': '<|endoftext|>', 'lm': '/leonardo_work/EUHPC_A04_051/alexfil/gpt2-chinese-cluecorpussmall', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.0, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 3, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 16, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [5, 6, 7, 8, 9, 10, 11], 'top_k': 2, 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.3, 'p_apply': 0.25, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': True, 'path_to_pretrained': '/leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth', 'feature_dims': [768, 33, 709], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 55, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': True, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.1, 'weight_decay_av': 0.1, 'learning_rate_av': 0.0002, 'learning_rate_mmgpt': 0.0002, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}
2026-01-18 14:54:20,796 - MMSA [INFO] - Seeds: [1990]
2026-01-18 14:54:20,797 - MMSA [INFO] - ------------------------------ Running with seed 1990 [1/1] ------------------------------
2026-01-18 14:55:39,717 - MMSA [INFO] - The model has 173499052 trainable parameters
2026-01-18 14:55:49,869 - MMSA [INFO] - TRAIN-(msalm) [1/1/1] >> loss: 0.5633  Mult_acc_2: 0.6923  Mult_acc_3: 0.5227  Mult_acc_5: 0.2573  F1_score: 0.6642  MAE: 0.5582  Corr: 0.2897  clm loss: 6.5772 total loss: 9.2783 bn loss: 0.726 av loss: 0.6165 text loss: 0.7953
2026-01-18 14:55:51,807 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7544  Mult_acc_3: 0.6053  Mult_acc_5: 0.3399  F1_score: 0.7435  MAE: 0.5243  Corr: 0.4676  Loss: 0.5684 
2026-01-18 14:55:55,005 - MMSA [INFO] - TEST-(msalm) >>  Mult_acc_2: 0.7724  Mult_acc_3: 0.6280  Mult_acc_5: 0.3611  F1_score: 0.7632  MAE: 0.4835  Corr: 0.5391  Loss: 0.5225 
2026-01-18 14:55:56,264 - MMSA [INFO] - Result for seed 1990: {'Mult_acc_2': 0.7724, 'Mult_acc_3': 0.628, 'Mult_acc_5': 0.3611, 'F1_score': 0.7632, 'MAE': 0.4835, 'Corr': 0.5391, 'Loss': 0.5225, 'seed': 1990}
2026-01-18 14:55:56,297 - MMSA [INFO] - Results saved to MMSA/results/sims/lrs_0002/sparse-mm-token-lvl-router/k_2/nf_16-track-exps/lr_0002__k_2__nf_16-gca-track-exps/normal/sims_avg.csv.
2026-01-18 14:58:16,197 - MMSA [INFO] - ======================================== Program Start ========================================
2026-01-18 14:58:16,269 - MMSA [INFO] - Running with args:
2026-01-18 14:58:16,269 - MMSA [INFO] - {'model_name': 'msalm', 'dataset_name': 'sims', 'featurePath': '/leonardo_work/EUHPC_A04_051/alexfil/MSA-Datasets/CH-SIMS/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 0, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 1, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'use_ulgm': False, 'update_labels_patience': 1, 'H': 3.0, 'del_model': True, 'max_token_len': 39, 'pad_token': '<|endoftext|>', 'lm': '/leonardo_work/EUHPC_A04_051/alexfil/gpt2-chinese-cluecorpussmall', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.0, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 3, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 16, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [5, 6, 7, 8, 9, 10, 11], 'top_k': 2, 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.3, 'p_apply': 0.25, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': True, 'path_to_pretrained': '/leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth', 'feature_dims': [768, 33, 709], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 55, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': True, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.1, 'weight_decay_av': 0.1, 'learning_rate_av': 0.0002, 'learning_rate_mmgpt': 0.0002, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}
2026-01-18 14:58:16,269 - MMSA [INFO] - Seeds: [1990]
2026-01-18 14:58:16,270 - MMSA [INFO] - ------------------------------ Running with seed 1990 [1/1] ------------------------------
2026-01-18 14:59:36,940 - MMSA [INFO] - The model has 173499052 trainable parameters
2026-01-18 14:59:47,070 - MMSA [INFO] - TRAIN-(msalm) [1/1/1] >> loss: 0.5633  Mult_acc_2: 0.6923  Mult_acc_3: 0.5227  Mult_acc_5: 0.2573  F1_score: 0.6642  MAE: 0.5582  Corr: 0.2897  clm loss: 6.5772 total loss: 9.2783 bn loss: 0.726 av loss: 0.6165 text loss: 0.7953
2026-01-18 14:59:48,996 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7544  Mult_acc_3: 0.6053  Mult_acc_5: 0.3399  F1_score: 0.7435  MAE: 0.5243  Corr: 0.4676  Loss: 0.5684 
2026-01-18 14:59:52,251 - MMSA [INFO] - TEST-(msalm) >>  Mult_acc_2: 0.7724  Mult_acc_3: 0.6280  Mult_acc_5: 0.3611  F1_score: 0.7632  MAE: 0.4835  Corr: 0.5391  Loss: 0.5225 
2026-01-18 14:59:53,524 - MMSA [INFO] - Result for seed 1990: {'Mult_acc_2': 0.7724, 'Mult_acc_3': 0.628, 'Mult_acc_5': 0.3611, 'F1_score': 0.7632, 'MAE': 0.4835, 'Corr': 0.5391, 'Loss': 0.5225, 'seed': 1990}
2026-01-18 14:59:53,557 - MMSA [INFO] - Results saved to MMSA/results/sims/lrs_0002/sparse-mm-token-lvl-router/k_2/nf_16-track-exps/lr_0002__k_2__nf_16-gca-track-exps/normal/sims_avg.csv.
2026-01-18 15:08:16,881 - MMSA [INFO] - ======================================== Program Start ========================================
2026-01-18 15:08:16,951 - MMSA [INFO] - Running with args:
2026-01-18 15:08:16,951 - MMSA [INFO] - {'model_name': 'msalm', 'dataset_name': 'sims', 'featurePath': '/leonardo_work/EUHPC_A04_051/alexfil/MSA-Datasets/CH-SIMS/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 0, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 1, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'use_ulgm': False, 'update_labels_patience': 1, 'H': 3.0, 'del_model': True, 'max_token_len': 39, 'pad_token': '<|endoftext|>', 'lm': '/leonardo_work/EUHPC_A04_051/alexfil/gpt2-chinese-cluecorpussmall', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.0, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 3, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 16, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [5, 6, 7, 8, 9, 10, 11], 'top_k': 2, 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.3, 'p_apply': 0.25, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': True, 'path_to_pretrained': '/leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth', 'feature_dims': [768, 33, 709], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 55, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': True, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.1, 'weight_decay_av': 0.1, 'learning_rate_av': 0.0002, 'learning_rate_mmgpt': 0.0002, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}
2026-01-18 15:08:16,951 - MMSA [INFO] - Seeds: [1990]
2026-01-18 15:08:16,952 - MMSA [INFO] - ------------------------------ Running with seed 1990 [1/1] ------------------------------
2026-01-18 15:09:36,905 - MMSA [INFO] - The model has 173499052 trainable parameters
2026-01-18 15:09:46,978 - MMSA [INFO] - TRAIN-(msalm) [1/1/1] >> loss: 0.5633  Mult_acc_2: 0.6923  Mult_acc_3: 0.5227  Mult_acc_5: 0.2573  F1_score: 0.6642  MAE: 0.5582  Corr: 0.2897  clm loss: 6.5772 total loss: 9.2783 bn loss: 0.726 av loss: 0.6165 text loss: 0.7953
2026-01-18 15:09:49,007 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7544  Mult_acc_3: 0.6053  Mult_acc_5: 0.3399  F1_score: 0.7435  MAE: 0.5243  Corr: 0.4676  Loss: 0.5684 
2026-01-18 15:09:52,235 - MMSA [INFO] - TEST-(msalm) >>  Mult_acc_2: 0.7724  Mult_acc_3: 0.6280  Mult_acc_5: 0.3611  F1_score: 0.7632  MAE: 0.4835  Corr: 0.5391  Loss: 0.5225 
2026-01-18 15:09:53,518 - MMSA [INFO] - Result for seed 1990: {'Mult_acc_2': 0.7724, 'Mult_acc_3': 0.628, 'Mult_acc_5': 0.3611, 'F1_score': 0.7632, 'MAE': 0.4835, 'Corr': 0.5391, 'Loss': 0.5225, 'seed': 1990}
2026-01-18 15:09:53,557 - MMSA [INFO] - Results saved to MMSA/results/sims/lrs_0002/sparse-mm-token-lvl-router/k_2/nf_16-track-exps/lr_0002__k_2__nf_16-gca-track-exps/normal/sims_avg.csv.
2026-01-18 15:15:59,403 - MMSA [INFO] - ======================================== Program Start ========================================
2026-01-18 15:15:59,466 - MMSA [INFO] - Running with args:
2026-01-18 15:15:59,466 - MMSA [INFO] - {'model_name': 'msalm', 'dataset_name': 'sims', 'featurePath': '/leonardo_work/EUHPC_A04_051/alexfil/MSA-Datasets/CH-SIMS/unaligned_39.pkl', 'seq_lens': [39, 400, 55], 'feature_dims': [768, 33, 709], 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'missing_rate': [0.0, 0.0, 0.0], 'missing_seed': [1111, 1111, 1111], 'need_data_aligned': False, 'need_model_aligned': False, 'early_stop': 0, 'use_bert': False, 'use_bert_finetune': False, 'attn_mask': True, 'excludeZero': True, 'update_epochs': 1, 'use_augmentation': False, 'use_m3xup': False, 'hfPath': False, 'use_ulgm': False, 'update_labels_patience': 1, 'H': 3.0, 'del_model': True, 'max_token_len': 39, 'pad_token': '<|endoftext|>', 'lm': '/leonardo_work/EUHPC_A04_051/alexfil/gpt2-chinese-cluecorpussmall', 'use_bf16': False, 'task_out': 1, 'use_clm': True, 'gamma': 1.0, 'l_bn': 1.0, 'l_av': 1.0, 'l_t': 1.0, 'warmup_epochs': 1, 'max_epochs': 3, 'beta_1': 0.9, 'beta_2': 0.95, 'use_lnorm': True, 'rescale': False, 'rescaler': 'sqrt', 'use_seqaug': True, 'n_bn_fusion': 16, 'modded_loss': True, 'embedding_attr_name': 'transformer.wte', 'decoder_layers_attr_name': 'transformer.h', 'mmgpt': {'type': 'gpt2', 'd_out': 64, 'combine': True, 'dropout': 0.1, 'mm_layer': [5, 6, 7, 8, 9, 10, 11], 'top_k': 2, 'layer_dropout': 0.0, 'dense': True, 'tie_ffn': True, 'n_embd': 768, 'bias': True, 'kv_dim': 30, 'n_head': 16, 'd_mm': 768, 'gating': 'sigmoid', 'init_gate': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'use_softperm': True, 'p_perm': 0.3, 'p_apply': 0.25, 'use_lora': False, 'lora': {'lora_alpha': 128, 'r': 64, 'lora_dropout': 0.05}}, 'av_enc': {'finetune': True, 'from_pretrained': True, 'path_to_pretrained': '/leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth', 'feature_dims': [768, 33, 709], 'd_enc': 30, 'n_embd': 30, 'n_head': 6, 'nlevels': 3, 'd_enc_out': 30, 'maxlen': 55, 'p_mask': 0.15, 'enc_attn_dropout': 0.1, 'enc_res_dropout': 0.1, 'enc_dropout': 0.1, 'use_softperm': True, 'p_perm': 0.2, 'mask_perm_ratio': 0.5, 'tf_fusion': False, 'use_bn': True, 'use_ln': False}, 'gpt': {'vocab_size': 50257, 'block_size': 1024, 'bias': True, 'n_embd': 1024, 'n_layer': 24, 'n_head': 16, 'dropout': 0.0}, 'batch_size': 32, 'grad_clip': 5.0, 'patience': 10, 'weight_decay_mmgpt': 0.1, 'weight_decay_av': 0.1, 'learning_rate_av': 0.0002, 'learning_rate_mmgpt': 0.0002, 'device': device(type='cuda', index=0), 'train_mode': 'regression', 'custom_feature': None, 'feature_T': '', 'feature_A': '', 'feature_V': ''}
2026-01-18 15:15:59,466 - MMSA [INFO] - Seeds: [1990]
2026-01-18 15:15:59,467 - MMSA [INFO] - ------------------------------ Running with seed 1990 [1/1] ------------------------------
2026-01-18 15:17:13,799 - MMSA [INFO] - The model has 173499052 trainable parameters
2026-01-18 15:17:23,091 - MMSA [INFO] - TRAIN-(msalm) [1/1/1] >> loss: 0.5633  Mult_acc_2: 0.6923  Mult_acc_3: 0.5227  Mult_acc_5: 0.2573  F1_score: 0.6642  MAE: 0.5582  Corr: 0.2897  clm loss: 6.5772 total loss: 9.2783 bn loss: 0.726 av loss: 0.6165 text loss: 0.7953
2026-01-18 15:17:24,919 - MMSA [INFO] - VAL-(msalm) >>  Mult_acc_2: 0.7544  Mult_acc_3: 0.6053  Mult_acc_5: 0.3399  F1_score: 0.7435  MAE: 0.5243  Corr: 0.4676  Loss: 0.5684 
2026-01-18 15:17:28,186 - MMSA [INFO] - TEST-(msalm) >>  Mult_acc_2: 0.7724  Mult_acc_3: 0.6280  Mult_acc_5: 0.3611  F1_score: 0.7632  MAE: 0.4835  Corr: 0.5391  Loss: 0.5225 
2026-01-18 15:17:29,455 - MMSA [INFO] - Result for seed 1990: {'Mult_acc_2': 0.7724, 'Mult_acc_3': 0.628, 'Mult_acc_5': 0.3611, 'F1_score': 0.7632, 'MAE': 0.4835, 'Corr': 0.5391, 'Loss': 0.5225, 'seed': 1990}
2026-01-18 15:17:29,470 - MMSA [INFO] - Results saved to MMSA/results/sims/lrs_0002/sparse-mm-token-lvl-router/k_2/nf_16-track-exps/lr_0002__k_2__nf_16-gca-track-exps/normal/sims_avg.csv.
