Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$Running in normal mode.$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Loading HF datasets
---------------------- Ongoing with TRAIN data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(1368, 400, 33)
(1368, 55, 709)
---------------------- Ongoing with VALID data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(456, 400, 33)
(456, 55, 709)
---------------------- Ongoing with TEST data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(457, 400, 33)
(457, 55, 709)
Ongoing with num_workers=2
ca list is: [5, 6, 7, 8, 9, 10, 11]
initializing SoftPerm
Ongoing with ----- sigmoid ----- gating
idx is 0
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 1
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 2
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 3
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 4
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 5
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 6
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Parsing decoder block: 0
Parsing decoder block: 1
Parsing decoder block: 2
Parsing decoder block: 3
Parsing decoder block: 4
Parsing decoder block: 5
COpying---------------------------------
Parsing decoder block: 6
COpying---------------------------------
Parsing decoder block: 7
COpying---------------------------------
Parsing decoder block: 8
COpying---------------------------------
Parsing decoder block: 9
COpying---------------------------------
Parsing decoder block: 10
COpying---------------------------------
Parsing decoder block: 11
COpying---------------------------------
Using BN_a
Using BN_v
----------------->>> Pretrained AudioVisual Encoder <<<<<----------------
Using BN_a
Using BN_v
----------------------- Loading AV encoder from /leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth
Copied param embed_positions_a._float_tensor
Copied param embed_positions_v._float_tensor
Copied param BN_a.weight
Copied param BN_a.bias
Copied param BN_a.running_mean
Copied param BN_a.running_var
Copied param BN_a.num_batches_tracked
Copied param BN_v.weight
Copied param BN_v.bias
Copied param BN_v.running_mean
Copied param BN_v.running_var
Copied param BN_v.num_batches_tracked
Copied param proj_a.weight
Copied param proj_v.weight
Copied param enc_a.layers.0.self_attn.in_proj_weight
Copied param enc_a.layers.0.self_attn.in_proj_bias
Copied param enc_a.layers.0.self_attn.out_proj.weight
Copied param enc_a.layers.0.self_attn.out_proj.bias
Copied param enc_a.layers.0.linear1.weight
Copied param enc_a.layers.0.linear1.bias
Copied param enc_a.layers.0.linear2.weight
Copied param enc_a.layers.0.linear2.bias
Copied param enc_a.layers.0.norm1.weight
Copied param enc_a.layers.0.norm1.bias
Copied param enc_a.layers.0.norm2.weight
Copied param enc_a.layers.0.norm2.bias
Copied param enc_a.layers.1.self_attn.in_proj_weight
Copied param enc_a.layers.1.self_attn.in_proj_bias
Copied param enc_a.layers.1.self_attn.out_proj.weight
Copied param enc_a.layers.1.self_attn.out_proj.bias
Copied param enc_a.layers.1.linear1.weight
Copied param enc_a.layers.1.linear1.bias
Copied param enc_a.layers.1.linear2.weight
Copied param enc_a.layers.1.linear2.bias
Copied param enc_a.layers.1.norm1.weight
Copied param enc_a.layers.1.norm1.bias
Copied param enc_a.layers.1.norm2.weight
Copied param enc_a.layers.1.norm2.bias
Copied param enc_a.layers.2.self_attn.in_proj_weight
Copied param enc_a.layers.2.self_attn.in_proj_bias
Copied param enc_a.layers.2.self_attn.out_proj.weight
Copied param enc_a.layers.2.self_attn.out_proj.bias
Copied param enc_a.layers.2.linear1.weight
Copied param enc_a.layers.2.linear1.bias
Copied param enc_a.layers.2.linear2.weight
Copied param enc_a.layers.2.linear2.bias
Copied param enc_a.layers.2.norm1.weight
Copied param enc_a.layers.2.norm1.bias
Copied param enc_a.layers.2.norm2.weight
Copied param enc_a.layers.2.norm2.bias
Copied param enc_v.layers.0.self_attn.in_proj_weight
Copied param enc_v.layers.0.self_attn.in_proj_bias
Copied param enc_v.layers.0.self_attn.out_proj.weight
Copied param enc_v.layers.0.self_attn.out_proj.bias
Copied param enc_v.layers.0.linear1.weight
Copied param enc_v.layers.0.linear1.bias
Copied param enc_v.layers.0.linear2.weight
Copied param enc_v.layers.0.linear2.bias
Copied param enc_v.layers.0.norm1.weight
Copied param enc_v.layers.0.norm1.bias
Copied param enc_v.layers.0.norm2.weight
Copied param enc_v.layers.0.norm2.bias
Copied param enc_v.layers.1.self_attn.in_proj_weight
Copied param enc_v.layers.1.self_attn.in_proj_bias
Copied param enc_v.layers.1.self_attn.out_proj.weight
Copied param enc_v.layers.1.self_attn.out_proj.bias
Copied param enc_v.layers.1.linear1.weight
Copied param enc_v.layers.1.linear1.bias
Copied param enc_v.layers.1.linear2.weight
Copied param enc_v.layers.1.linear2.bias
Copied param enc_v.layers.1.norm1.weight
Copied param enc_v.layers.1.norm1.bias
Copied param enc_v.layers.1.norm2.weight
Copied param enc_v.layers.1.norm2.bias
Copied param enc_v.layers.2.self_attn.in_proj_weight
Copied param enc_v.layers.2.self_attn.in_proj_bias
Copied param enc_v.layers.2.self_attn.out_proj.weight
Copied param enc_v.layers.2.self_attn.out_proj.bias
Copied param enc_v.layers.2.linear1.weight
Copied param enc_v.layers.2.linear1.bias
Copied param enc_v.layers.2.linear2.weight
Copied param enc_v.layers.2.linear2.bias
Copied param enc_v.layers.2.norm1.weight
Copied param enc_v.layers.2.norm1.bias
Copied param enc_v.layers.2.norm2.weight
Copied param enc_v.layers.2.norm2.bias
Copied param fusion.weight
Copied param fusion.bias
Copied param clf.weight
Copied param clf.bias
------------------ Adding LNorm ------------------------
ongoing with msalm
5.ca_layer.alpha_1
5.ca_layer.alpha_2
5.ca_layer.audio_expert.attn.W_q.weight
5.ca_layer.audio_expert.attn.W_kv.weight
5.ca_layer.audio_expert.attn.W_o.weight
5.ca_layer.visual_expert.attn.W_q.weight
5.ca_layer.visual_expert.attn.W_kv.weight
5.ca_layer.visual_expert.attn.W_o.weight
5.ca_layer.av_expert.attn.W_q.weight
5.ca_layer.av_expert.attn.W_kv.weight
5.ca_layer.av_expert.attn.W_o.weight
5.ca_layer.ln_1.weight
5.ca_layer.ln_1.bias
5.ca_layer.ln_2.weight
5.ca_layer.ln_2.bias
5.ca_layer.mlp.c_fc.weight
5.ca_layer.mlp.c_fc.bias
5.ca_layer.mlp.c_proj.weight
5.ca_layer.mlp.c_proj.bias
6.ca_layer.alpha_1
6.ca_layer.alpha_2
6.ca_layer.audio_expert.attn.W_q.weight
6.ca_layer.audio_expert.attn.W_kv.weight
6.ca_layer.audio_expert.attn.W_o.weight
6.ca_layer.visual_expert.attn.W_q.weight
6.ca_layer.visual_expert.attn.W_kv.weight
6.ca_layer.visual_expert.attn.W_o.weight
6.ca_layer.av_expert.attn.W_q.weight
6.ca_layer.av_expert.attn.W_kv.weight
6.ca_layer.av_expert.attn.W_o.weight
6.ca_layer.ln_1.weight
6.ca_layer.ln_1.bias
6.ca_layer.ln_2.weight
6.ca_layer.ln_2.bias
6.ca_layer.mlp.c_fc.weight
6.ca_layer.mlp.c_fc.bias
6.ca_layer.mlp.c_proj.weight
6.ca_layer.mlp.c_proj.bias
7.ca_layer.alpha_1
7.ca_layer.alpha_2
7.ca_layer.audio_expert.attn.W_q.weight
7.ca_layer.audio_expert.attn.W_kv.weight
7.ca_layer.audio_expert.attn.W_o.weight
7.ca_layer.visual_expert.attn.W_q.weight
7.ca_layer.visual_expert.attn.W_kv.weight
7.ca_layer.visual_expert.attn.W_o.weight
7.ca_layer.av_expert.attn.W_q.weight
7.ca_layer.av_expert.attn.W_kv.weight
7.ca_layer.av_expert.attn.W_o.weight
7.ca_layer.ln_1.weight
7.ca_layer.ln_1.bias
7.ca_layer.ln_2.weight
7.ca_layer.ln_2.bias
7.ca_layer.mlp.c_fc.weight
7.ca_layer.mlp.c_fc.bias
7.ca_layer.mlp.c_proj.weight
7.ca_layer.mlp.c_proj.bias
8.ca_layer.alpha_1
8.ca_layer.alpha_2
8.ca_layer.audio_expert.attn.W_q.weight
8.ca_layer.audio_expert.attn.W_kv.weight
8.ca_layer.audio_expert.attn.W_o.weight
8.ca_layer.visual_expert.attn.W_q.weight
8.ca_layer.visual_expert.attn.W_kv.weight
8.ca_layer.visual_expert.attn.W_o.weight
8.ca_layer.av_expert.attn.W_q.weight
8.ca_layer.av_expert.attn.W_kv.weight
8.ca_layer.av_expert.attn.W_o.weight
8.ca_layer.ln_1.weight
8.ca_layer.ln_1.bias
8.ca_layer.ln_2.weight
8.ca_layer.ln_2.bias
8.ca_layer.mlp.c_fc.weight
8.ca_layer.mlp.c_fc.bias
8.ca_layer.mlp.c_proj.weight
8.ca_layer.mlp.c_proj.bias
9.ca_layer.alpha_1
9.ca_layer.alpha_2
9.ca_layer.audio_expert.attn.W_q.weight
9.ca_layer.audio_expert.attn.W_kv.weight
9.ca_layer.audio_expert.attn.W_o.weight
9.ca_layer.visual_expert.attn.W_q.weight
9.ca_layer.visual_expert.attn.W_kv.weight
9.ca_layer.visual_expert.attn.W_o.weight
9.ca_layer.av_expert.attn.W_q.weight
9.ca_layer.av_expert.attn.W_kv.weight
9.ca_layer.av_expert.attn.W_o.weight
9.ca_layer.ln_1.weight
9.ca_layer.ln_1.bias
9.ca_layer.ln_2.weight
9.ca_layer.ln_2.bias
9.ca_layer.mlp.c_fc.weight
9.ca_layer.mlp.c_fc.bias
9.ca_layer.mlp.c_proj.weight
9.ca_layer.mlp.c_proj.bias
10.ca_layer.alpha_1
10.ca_layer.alpha_2
10.ca_layer.audio_expert.attn.W_q.weight
10.ca_layer.audio_expert.attn.W_kv.weight
10.ca_layer.audio_expert.attn.W_o.weight
10.ca_layer.visual_expert.attn.W_q.weight
10.ca_layer.visual_expert.attn.W_kv.weight
10.ca_layer.visual_expert.attn.W_o.weight
10.ca_layer.av_expert.attn.W_q.weight
10.ca_layer.av_expert.attn.W_kv.weight
10.ca_layer.av_expert.attn.W_o.weight
10.ca_layer.ln_1.weight
10.ca_layer.ln_1.bias
10.ca_layer.ln_2.weight
10.ca_layer.ln_2.bias
10.ca_layer.mlp.c_fc.weight
10.ca_layer.mlp.c_fc.bias
10.ca_layer.mlp.c_proj.weight
10.ca_layer.mlp.c_proj.bias
11.ca_layer.alpha_1
11.ca_layer.alpha_2
11.ca_layer.audio_expert.attn.W_q.weight
11.ca_layer.audio_expert.attn.W_kv.weight
11.ca_layer.audio_expert.attn.W_o.weight
11.ca_layer.visual_expert.attn.W_q.weight
11.ca_layer.visual_expert.attn.W_kv.weight
11.ca_layer.visual_expert.attn.W_o.weight
11.ca_layer.av_expert.attn.W_q.weight
11.ca_layer.av_expert.attn.W_kv.weight
11.ca_layer.av_expert.attn.W_o.weight
11.ca_layer.ln_1.weight
11.ca_layer.ln_1.bias
11.ca_layer.ln_2.weight
11.ca_layer.ln_2.bias
11.ca_layer.mlp.c_fc.weight
11.ca_layer.mlp.c_fc.bias
11.ca_layer.mlp.c_proj.weight
11.ca_layer.mlp.c_proj.bias
0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Model.W_task.0.weight
Model.W_task.0.bias
Model.W_task.1.weight
Model.W_task.1.bias
Model.W_task.3.weight
Model.W_task.3.bias
Model.W_bn.weight
Model.W_bn.bias
Model.W_text.weight
Model.W_text.bias
Model.W_av.weight
Model.W_av.bias
Model.av_encoder.BN_a.weight
Model.av_encoder.BN_a.bias
Model.av_encoder.BN_v.weight
Model.av_encoder.BN_v.bias
Model.av_encoder.proj_a.weight
Model.av_encoder.proj_v.weight
Model.av_encoder.enc_a.layers.0.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.0.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.0.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.0.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.0.linear1.weight
Model.av_encoder.enc_a.layers.0.linear1.bias
Model.av_encoder.enc_a.layers.0.linear2.weight
Model.av_encoder.enc_a.layers.0.linear2.bias
Model.av_encoder.enc_a.layers.0.norm1.weight
Model.av_encoder.enc_a.layers.0.norm1.bias
Model.av_encoder.enc_a.layers.0.norm2.weight
Model.av_encoder.enc_a.layers.0.norm2.bias
Model.av_encoder.enc_a.layers.1.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.1.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.1.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.1.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.1.linear1.weight
Model.av_encoder.enc_a.layers.1.linear1.bias
Model.av_encoder.enc_a.layers.1.linear2.weight
Model.av_encoder.enc_a.layers.1.linear2.bias
Model.av_encoder.enc_a.layers.1.norm1.weight
Model.av_encoder.enc_a.layers.1.norm1.bias
Model.av_encoder.enc_a.layers.1.norm2.weight
Model.av_encoder.enc_a.layers.1.norm2.bias
Model.av_encoder.enc_a.layers.2.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.2.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.2.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.2.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.2.linear1.weight
Model.av_encoder.enc_a.layers.2.linear1.bias
Model.av_encoder.enc_a.layers.2.linear2.weight
Model.av_encoder.enc_a.layers.2.linear2.bias
Model.av_encoder.enc_a.layers.2.norm1.weight
Model.av_encoder.enc_a.layers.2.norm1.bias
Model.av_encoder.enc_a.layers.2.norm2.weight
Model.av_encoder.enc_a.layers.2.norm2.bias
Model.av_encoder.enc_v.layers.0.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.0.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.0.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.0.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.0.linear1.weight
Model.av_encoder.enc_v.layers.0.linear1.bias
Model.av_encoder.enc_v.layers.0.linear2.weight
Model.av_encoder.enc_v.layers.0.linear2.bias
Model.av_encoder.enc_v.layers.0.norm1.weight
Model.av_encoder.enc_v.layers.0.norm1.bias
Model.av_encoder.enc_v.layers.0.norm2.weight
Model.av_encoder.enc_v.layers.0.norm2.bias
Model.av_encoder.enc_v.layers.1.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.1.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.1.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.1.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.1.linear1.weight
Model.av_encoder.enc_v.layers.1.linear1.bias
Model.av_encoder.enc_v.layers.1.linear2.weight
Model.av_encoder.enc_v.layers.1.linear2.bias
Model.av_encoder.enc_v.layers.1.norm1.weight
Model.av_encoder.enc_v.layers.1.norm1.bias
Model.av_encoder.enc_v.layers.1.norm2.weight
Model.av_encoder.enc_v.layers.1.norm2.bias
Model.av_encoder.enc_v.layers.2.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.2.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.2.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.2.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.2.linear1.weight
Model.av_encoder.enc_v.layers.2.linear1.bias
Model.av_encoder.enc_v.layers.2.linear2.weight
Model.av_encoder.enc_v.layers.2.linear2.bias
Model.av_encoder.enc_v.layers.2.norm1.weight
Model.av_encoder.enc_v.layers.2.norm1.bias
Model.av_encoder.enc_v.layers.2.norm2.weight
Model.av_encoder.enc_v.layers.2.norm2.bias
Model.av_encoder.fusion.weight
Model.av_encoder.fusion.bias
Model.av_encoder.clf.weight
Model.av_encoder.clf.bias
Model.LN.weight
Model.LN.bias
The total number of trainable parameters is 59.03 M
Model.lang_encoder.transformer.wte.0.embedding.weight
Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Using grad with decay in Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.wpe.0.positional.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.5.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.6.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.7.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.8.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.9.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.10.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.11.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.ln_f.weight
Model.lang_encoder.transformer.ln_f.bias
Model.W_task.0.weight
Using grad with decay in Model.W_task.0.weight
Model.W_task.0.bias
Using grad with no decay in Model.W_task.0.bias
Model.W_task.1.weight
Using grad with decay in Model.W_task.1.weight
Model.W_task.1.bias
Using grad with no decay in Model.W_task.1.bias
Model.W_task.3.weight
Using grad with decay in Model.W_task.3.weight
Model.W_task.3.bias
Using grad with no decay in Model.W_task.3.bias
Model.W_bn.weight
Using grad with decay in Model.W_bn.weight
Model.W_bn.bias
Using grad with no decay in Model.W_bn.bias
Model.W_text.weight
Using grad with decay in Model.W_text.weight
Model.W_text.bias
Using grad with no decay in Model.W_text.bias
Model.W_av.weight
Using grad with decay in Model.W_av.weight
Model.W_av.bias
Using grad with no decay in Model.W_av.bias
Model.LN.weight
Using grad with decay in Model.LN.weight
Model.LN.bias
Using grad with no decay in Model.LN.bias
Will be using warmup for 5 steps
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([1.7972e-05], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([9.9527e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([8.9772e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([3.1184e-06], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-6.1178e-05], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-6.5145e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-7.3386e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-7.0873e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([7.2619e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-7.7149e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-2.6233e-05], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-3.1275e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([5.3137e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-7.4113e-06], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-7.5709e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([9.6474e-05], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([5.2761e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([6.2340e-06], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([2.0579e-06], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0013], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([8.7985e-05], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0013], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0024], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0022], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([5.6492e-05], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0027], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0023], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-5.5237e-05], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([6.5698e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([1.6958e-05], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0031], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([7.2881e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0024], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-5.4026e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([7.0521e-05], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0013], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0033], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([8.1841e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0024], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([1.8716e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-7.8249e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0036], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([3.6209e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0023], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([3.4311e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([5.4547e-05], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0037], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([2.6168e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0022], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([5.3696e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0039], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([1.5293e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([9.7905e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([9.6589e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([4.4445e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0024], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([8.2379e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-9.2409e-05], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0026], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0042], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-7.8689e-07], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0030], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0033], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0044], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0036], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0045], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0038], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0046], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0048], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0045], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0048], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0022], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0046], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0048], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0022], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0048], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0048], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0050], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0052], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0045], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0053], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0054], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0054], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0037], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0013], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0054], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0057], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0034], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0053], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0074], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0031], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0053], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0071], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0028], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0052], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0064], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0081], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0025], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-1.8923e-05], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0051], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0057], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0072], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0022], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0050], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0069], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0088], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0079], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0075], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0049], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0072], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0092], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0082], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0079], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0069], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0048], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0075], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0095], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0085], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0082], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0072], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0099], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0088], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0086], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0076], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0046], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0080], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0102], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0091], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0089], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0075], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0079], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0045], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0083], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0105], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0094], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0092], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0082], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-9.0916e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0086], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0108], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0096], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0096], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0080], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0085], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0076], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0088], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0111], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0098], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0099], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0082], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0088], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0039], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0078], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0090], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0114], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0101], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0102], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0091], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0037], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0081], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0093], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0117], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0103], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0105], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0086], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0094], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0035], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0095], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0120], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0106], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0108], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0088], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0098], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0032], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0086], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0097], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0123], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0108], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0111], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0090], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0101], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0057], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0030], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0088], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0099], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0126], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0110], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0114], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0092], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0103], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0028], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0091], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0101], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0128], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0112], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0117], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0094], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0106], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0025], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0093], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0103], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0131], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0114], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0119], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0096], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0109], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0023], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0095], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0105], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0134], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0116], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0122], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0099], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0111], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0097], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0108], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0136], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0118], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0124], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0101], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0114], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0100], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0110], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0138], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0120], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0127], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0103], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0116], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0102], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0111], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0141], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0122], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0129], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0105], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0119], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0103], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0113], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0143], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0124], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0131], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0107], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0121], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0069], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0106], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0114], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0145], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0125], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0134], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0109], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0124], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0057], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0108], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0116], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0148], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0127], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0136], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0110], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0126], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0071], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0064], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0109], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0117], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0149], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0129], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0138], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0112], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0128], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0072], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0111], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0119], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0151], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0131], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0140], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0114], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0130], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0074], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0074], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0113], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0121], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0153], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0132], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0142], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0116], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0132], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0076], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0076], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0114], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0122], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0155], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0134], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0144], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0117], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0134], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0078], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0069], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0116], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0123], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0157], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0135], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0145], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0119], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0135], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0078], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0064], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0079], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0057], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([5.9957e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0118], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0125], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0159], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0136], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0147], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0120], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0137], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0079], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0081], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0071], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0119], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0126], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0160], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0138], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0149], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0121], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0139], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0080], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0083], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0072], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0121], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0127], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0162], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0139], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0150], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0123], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0140], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0081], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0072], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0122], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0128], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0163], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0140], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0152], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0124], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0142], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0082], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0086], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0123], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0129], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0165], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0141], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0153], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0125], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0143], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0083], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0087], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0074], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0124], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0130], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0166], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0142], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0154], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0126], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0145], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0069], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0089], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0075], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0126], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0131], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0167], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0144], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0156], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0127], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0146], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0085], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0090], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0076], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0127], device='cuda:0', requires_grad=True)
***************** Loading Model from checkpoints/lr_0001__k_2__nf_16-v2/msalm-sims-1990.pth
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0119], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0151], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0131], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0140], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0114], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0130], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0074], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0074], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0113], device='cuda:0', requires_grad=True)
Deleting stored model from checkpoints/lr_0001__k_2__nf_16-v2/msalm-sims-1990.pth
Loading HF datasets
---------------------- Ongoing with TRAIN data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(1368, 400, 33)
(1368, 55, 709)
---------------------- Ongoing with VALID data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(456, 400, 33)
(456, 55, 709)
---------------------- Ongoing with TEST data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(457, 400, 33)
(457, 55, 709)
Ongoing with num_workers=2
ca list is: [5, 6, 7, 8, 9, 10, 11]
initializing SoftPerm
Ongoing with ----- sigmoid ----- gating
idx is 0
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 1
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 2
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 3
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 4
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 5
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 6
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Parsing decoder block: 0
Parsing decoder block: 1
Parsing decoder block: 2
Parsing decoder block: 3
Parsing decoder block: 4
Parsing decoder block: 5
COpying---------------------------------
Parsing decoder block: 6
COpying---------------------------------
Parsing decoder block: 7
COpying---------------------------------
Parsing decoder block: 8
COpying---------------------------------
Parsing decoder block: 9
COpying---------------------------------
Parsing decoder block: 10
COpying---------------------------------
Parsing decoder block: 11
COpying---------------------------------
Using BN_a
Using BN_v
----------------->>> Pretrained AudioVisual Encoder <<<<<----------------
Using BN_a
Using BN_v
----------------------- Loading AV encoder from /leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth
Copied param embed_positions_a._float_tensor
Copied param embed_positions_v._float_tensor
Copied param BN_a.weight
Copied param BN_a.bias
Copied param BN_a.running_mean
Copied param BN_a.running_var
Copied param BN_a.num_batches_tracked
Copied param BN_v.weight
Copied param BN_v.bias
Copied param BN_v.running_mean
Copied param BN_v.running_var
Copied param BN_v.num_batches_tracked
Copied param proj_a.weight
Copied param proj_v.weight
Copied param enc_a.layers.0.self_attn.in_proj_weight
Copied param enc_a.layers.0.self_attn.in_proj_bias
Copied param enc_a.layers.0.self_attn.out_proj.weight
Copied param enc_a.layers.0.self_attn.out_proj.bias
Copied param enc_a.layers.0.linear1.weight
Copied param enc_a.layers.0.linear1.bias
Copied param enc_a.layers.0.linear2.weight
Copied param enc_a.layers.0.linear2.bias
Copied param enc_a.layers.0.norm1.weight
Copied param enc_a.layers.0.norm1.bias
Copied param enc_a.layers.0.norm2.weight
Copied param enc_a.layers.0.norm2.bias
Copied param enc_a.layers.1.self_attn.in_proj_weight
Copied param enc_a.layers.1.self_attn.in_proj_bias
Copied param enc_a.layers.1.self_attn.out_proj.weight
Copied param enc_a.layers.1.self_attn.out_proj.bias
Copied param enc_a.layers.1.linear1.weight
Copied param enc_a.layers.1.linear1.bias
Copied param enc_a.layers.1.linear2.weight
Copied param enc_a.layers.1.linear2.bias
Copied param enc_a.layers.1.norm1.weight
Copied param enc_a.layers.1.norm1.bias
Copied param enc_a.layers.1.norm2.weight
Copied param enc_a.layers.1.norm2.bias
Copied param enc_a.layers.2.self_attn.in_proj_weight
Copied param enc_a.layers.2.self_attn.in_proj_bias
Copied param enc_a.layers.2.self_attn.out_proj.weight
Copied param enc_a.layers.2.self_attn.out_proj.bias
Copied param enc_a.layers.2.linear1.weight
Copied param enc_a.layers.2.linear1.bias
Copied param enc_a.layers.2.linear2.weight
Copied param enc_a.layers.2.linear2.bias
Copied param enc_a.layers.2.norm1.weight
Copied param enc_a.layers.2.norm1.bias
Copied param enc_a.layers.2.norm2.weight
Copied param enc_a.layers.2.norm2.bias
Copied param enc_v.layers.0.self_attn.in_proj_weight
Copied param enc_v.layers.0.self_attn.in_proj_bias
Copied param enc_v.layers.0.self_attn.out_proj.weight
Copied param enc_v.layers.0.self_attn.out_proj.bias
Copied param enc_v.layers.0.linear1.weight
Copied param enc_v.layers.0.linear1.bias
Copied param enc_v.layers.0.linear2.weight
Copied param enc_v.layers.0.linear2.bias
Copied param enc_v.layers.0.norm1.weight
Copied param enc_v.layers.0.norm1.bias
Copied param enc_v.layers.0.norm2.weight
Copied param enc_v.layers.0.norm2.bias
Copied param enc_v.layers.1.self_attn.in_proj_weight
Copied param enc_v.layers.1.self_attn.in_proj_bias
Copied param enc_v.layers.1.self_attn.out_proj.weight
Copied param enc_v.layers.1.self_attn.out_proj.bias
Copied param enc_v.layers.1.linear1.weight
Copied param enc_v.layers.1.linear1.bias
Copied param enc_v.layers.1.linear2.weight
Copied param enc_v.layers.1.linear2.bias
Copied param enc_v.layers.1.norm1.weight
Copied param enc_v.layers.1.norm1.bias
Copied param enc_v.layers.1.norm2.weight
Copied param enc_v.layers.1.norm2.bias
Copied param enc_v.layers.2.self_attn.in_proj_weight
Copied param enc_v.layers.2.self_attn.in_proj_bias
Copied param enc_v.layers.2.self_attn.out_proj.weight
Copied param enc_v.layers.2.self_attn.out_proj.bias
Copied param enc_v.layers.2.linear1.weight
Copied param enc_v.layers.2.linear1.bias
Copied param enc_v.layers.2.linear2.weight
Copied param enc_v.layers.2.linear2.bias
Copied param enc_v.layers.2.norm1.weight
Copied param enc_v.layers.2.norm1.bias
Copied param enc_v.layers.2.norm2.weight
Copied param enc_v.layers.2.norm2.bias
Copied param fusion.weight
Copied param fusion.bias
Copied param clf.weight
Copied param clf.bias
------------------ Adding LNorm ------------------------
ongoing with msalm
5.ca_layer.alpha_1
5.ca_layer.alpha_2
5.ca_layer.audio_expert.attn.W_q.weight
5.ca_layer.audio_expert.attn.W_kv.weight
5.ca_layer.audio_expert.attn.W_o.weight
5.ca_layer.visual_expert.attn.W_q.weight
5.ca_layer.visual_expert.attn.W_kv.weight
5.ca_layer.visual_expert.attn.W_o.weight
5.ca_layer.av_expert.attn.W_q.weight
5.ca_layer.av_expert.attn.W_kv.weight
5.ca_layer.av_expert.attn.W_o.weight
5.ca_layer.ln_1.weight
5.ca_layer.ln_1.bias
5.ca_layer.ln_2.weight
5.ca_layer.ln_2.bias
5.ca_layer.mlp.c_fc.weight
5.ca_layer.mlp.c_fc.bias
5.ca_layer.mlp.c_proj.weight
5.ca_layer.mlp.c_proj.bias
6.ca_layer.alpha_1
6.ca_layer.alpha_2
6.ca_layer.audio_expert.attn.W_q.weight
6.ca_layer.audio_expert.attn.W_kv.weight
6.ca_layer.audio_expert.attn.W_o.weight
6.ca_layer.visual_expert.attn.W_q.weight
6.ca_layer.visual_expert.attn.W_kv.weight
6.ca_layer.visual_expert.attn.W_o.weight
6.ca_layer.av_expert.attn.W_q.weight
6.ca_layer.av_expert.attn.W_kv.weight
6.ca_layer.av_expert.attn.W_o.weight
6.ca_layer.ln_1.weight
6.ca_layer.ln_1.bias
6.ca_layer.ln_2.weight
6.ca_layer.ln_2.bias
6.ca_layer.mlp.c_fc.weight
6.ca_layer.mlp.c_fc.bias
6.ca_layer.mlp.c_proj.weight
6.ca_layer.mlp.c_proj.bias
7.ca_layer.alpha_1
7.ca_layer.alpha_2
7.ca_layer.audio_expert.attn.W_q.weight
7.ca_layer.audio_expert.attn.W_kv.weight
7.ca_layer.audio_expert.attn.W_o.weight
7.ca_layer.visual_expert.attn.W_q.weight
7.ca_layer.visual_expert.attn.W_kv.weight
7.ca_layer.visual_expert.attn.W_o.weight
7.ca_layer.av_expert.attn.W_q.weight
7.ca_layer.av_expert.attn.W_kv.weight
7.ca_layer.av_expert.attn.W_o.weight
7.ca_layer.ln_1.weight
7.ca_layer.ln_1.bias
7.ca_layer.ln_2.weight
7.ca_layer.ln_2.bias
7.ca_layer.mlp.c_fc.weight
7.ca_layer.mlp.c_fc.bias
7.ca_layer.mlp.c_proj.weight
7.ca_layer.mlp.c_proj.bias
8.ca_layer.alpha_1
8.ca_layer.alpha_2
8.ca_layer.audio_expert.attn.W_q.weight
8.ca_layer.audio_expert.attn.W_kv.weight
8.ca_layer.audio_expert.attn.W_o.weight
8.ca_layer.visual_expert.attn.W_q.weight
8.ca_layer.visual_expert.attn.W_kv.weight
8.ca_layer.visual_expert.attn.W_o.weight
8.ca_layer.av_expert.attn.W_q.weight
8.ca_layer.av_expert.attn.W_kv.weight
8.ca_layer.av_expert.attn.W_o.weight
8.ca_layer.ln_1.weight
8.ca_layer.ln_1.bias
8.ca_layer.ln_2.weight
8.ca_layer.ln_2.bias
8.ca_layer.mlp.c_fc.weight
8.ca_layer.mlp.c_fc.bias
8.ca_layer.mlp.c_proj.weight
8.ca_layer.mlp.c_proj.bias
9.ca_layer.alpha_1
9.ca_layer.alpha_2
9.ca_layer.audio_expert.attn.W_q.weight
9.ca_layer.audio_expert.attn.W_kv.weight
9.ca_layer.audio_expert.attn.W_o.weight
9.ca_layer.visual_expert.attn.W_q.weight
9.ca_layer.visual_expert.attn.W_kv.weight
9.ca_layer.visual_expert.attn.W_o.weight
9.ca_layer.av_expert.attn.W_q.weight
9.ca_layer.av_expert.attn.W_kv.weight
9.ca_layer.av_expert.attn.W_o.weight
9.ca_layer.ln_1.weight
9.ca_layer.ln_1.bias
9.ca_layer.ln_2.weight
9.ca_layer.ln_2.bias
9.ca_layer.mlp.c_fc.weight
9.ca_layer.mlp.c_fc.bias
9.ca_layer.mlp.c_proj.weight
9.ca_layer.mlp.c_proj.bias
10.ca_layer.alpha_1
10.ca_layer.alpha_2
10.ca_layer.audio_expert.attn.W_q.weight
10.ca_layer.audio_expert.attn.W_kv.weight
10.ca_layer.audio_expert.attn.W_o.weight
10.ca_layer.visual_expert.attn.W_q.weight
10.ca_layer.visual_expert.attn.W_kv.weight
10.ca_layer.visual_expert.attn.W_o.weight
10.ca_layer.av_expert.attn.W_q.weight
10.ca_layer.av_expert.attn.W_kv.weight
10.ca_layer.av_expert.attn.W_o.weight
10.ca_layer.ln_1.weight
10.ca_layer.ln_1.bias
10.ca_layer.ln_2.weight
10.ca_layer.ln_2.bias
10.ca_layer.mlp.c_fc.weight
10.ca_layer.mlp.c_fc.bias
10.ca_layer.mlp.c_proj.weight
10.ca_layer.mlp.c_proj.bias
11.ca_layer.alpha_1
11.ca_layer.alpha_2
11.ca_layer.audio_expert.attn.W_q.weight
11.ca_layer.audio_expert.attn.W_kv.weight
11.ca_layer.audio_expert.attn.W_o.weight
11.ca_layer.visual_expert.attn.W_q.weight
11.ca_layer.visual_expert.attn.W_kv.weight
11.ca_layer.visual_expert.attn.W_o.weight
11.ca_layer.av_expert.attn.W_q.weight
11.ca_layer.av_expert.attn.W_kv.weight
11.ca_layer.av_expert.attn.W_o.weight
11.ca_layer.ln_1.weight
11.ca_layer.ln_1.bias
11.ca_layer.ln_2.weight
11.ca_layer.ln_2.bias
11.ca_layer.mlp.c_fc.weight
11.ca_layer.mlp.c_fc.bias
11.ca_layer.mlp.c_proj.weight
11.ca_layer.mlp.c_proj.bias
0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Model.W_task.0.weight
Model.W_task.0.bias
Model.W_task.1.weight
Model.W_task.1.bias
Model.W_task.3.weight
Model.W_task.3.bias
Model.W_bn.weight
Model.W_bn.bias
Model.W_text.weight
Model.W_text.bias
Model.W_av.weight
Model.W_av.bias
Model.av_encoder.BN_a.weight
Model.av_encoder.BN_a.bias
Model.av_encoder.BN_v.weight
Model.av_encoder.BN_v.bias
Model.av_encoder.proj_a.weight
Model.av_encoder.proj_v.weight
Model.av_encoder.enc_a.layers.0.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.0.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.0.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.0.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.0.linear1.weight
Model.av_encoder.enc_a.layers.0.linear1.bias
Model.av_encoder.enc_a.layers.0.linear2.weight
Model.av_encoder.enc_a.layers.0.linear2.bias
Model.av_encoder.enc_a.layers.0.norm1.weight
Model.av_encoder.enc_a.layers.0.norm1.bias
Model.av_encoder.enc_a.layers.0.norm2.weight
Model.av_encoder.enc_a.layers.0.norm2.bias
Model.av_encoder.enc_a.layers.1.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.1.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.1.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.1.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.1.linear1.weight
Model.av_encoder.enc_a.layers.1.linear1.bias
Model.av_encoder.enc_a.layers.1.linear2.weight
Model.av_encoder.enc_a.layers.1.linear2.bias
Model.av_encoder.enc_a.layers.1.norm1.weight
Model.av_encoder.enc_a.layers.1.norm1.bias
Model.av_encoder.enc_a.layers.1.norm2.weight
Model.av_encoder.enc_a.layers.1.norm2.bias
Model.av_encoder.enc_a.layers.2.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.2.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.2.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.2.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.2.linear1.weight
Model.av_encoder.enc_a.layers.2.linear1.bias
Model.av_encoder.enc_a.layers.2.linear2.weight
Model.av_encoder.enc_a.layers.2.linear2.bias
Model.av_encoder.enc_a.layers.2.norm1.weight
Model.av_encoder.enc_a.layers.2.norm1.bias
Model.av_encoder.enc_a.layers.2.norm2.weight
Model.av_encoder.enc_a.layers.2.norm2.bias
Model.av_encoder.enc_v.layers.0.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.0.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.0.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.0.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.0.linear1.weight
Model.av_encoder.enc_v.layers.0.linear1.bias
Model.av_encoder.enc_v.layers.0.linear2.weight
Model.av_encoder.enc_v.layers.0.linear2.bias
Model.av_encoder.enc_v.layers.0.norm1.weight
Model.av_encoder.enc_v.layers.0.norm1.bias
Model.av_encoder.enc_v.layers.0.norm2.weight
Model.av_encoder.enc_v.layers.0.norm2.bias
Model.av_encoder.enc_v.layers.1.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.1.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.1.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.1.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.1.linear1.weight
Model.av_encoder.enc_v.layers.1.linear1.bias
Model.av_encoder.enc_v.layers.1.linear2.weight
Model.av_encoder.enc_v.layers.1.linear2.bias
Model.av_encoder.enc_v.layers.1.norm1.weight
Model.av_encoder.enc_v.layers.1.norm1.bias
Model.av_encoder.enc_v.layers.1.norm2.weight
Model.av_encoder.enc_v.layers.1.norm2.bias
Model.av_encoder.enc_v.layers.2.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.2.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.2.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.2.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.2.linear1.weight
Model.av_encoder.enc_v.layers.2.linear1.bias
Model.av_encoder.enc_v.layers.2.linear2.weight
Model.av_encoder.enc_v.layers.2.linear2.bias
Model.av_encoder.enc_v.layers.2.norm1.weight
Model.av_encoder.enc_v.layers.2.norm1.bias
Model.av_encoder.enc_v.layers.2.norm2.weight
Model.av_encoder.enc_v.layers.2.norm2.bias
Model.av_encoder.fusion.weight
Model.av_encoder.fusion.bias
Model.av_encoder.clf.weight
Model.av_encoder.clf.bias
Model.LN.weight
Model.LN.bias
The total number of trainable parameters is 59.03 M
Model.lang_encoder.transformer.wte.0.embedding.weight
Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Using grad with decay in Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.wpe.0.positional.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.5.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.6.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.7.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.8.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.9.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.10.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.11.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.ln_f.weight
Model.lang_encoder.transformer.ln_f.bias
Model.W_task.0.weight
Using grad with decay in Model.W_task.0.weight
Model.W_task.0.bias
Using grad with no decay in Model.W_task.0.bias
Model.W_task.1.weight
Using grad with decay in Model.W_task.1.weight
Model.W_task.1.bias
Using grad with no decay in Model.W_task.1.bias
Model.W_task.3.weight
Using grad with decay in Model.W_task.3.weight
Model.W_task.3.bias
Using grad with no decay in Model.W_task.3.bias
Model.W_bn.weight
Using grad with decay in Model.W_bn.weight
Model.W_bn.bias
Using grad with no decay in Model.W_bn.bias
Model.W_text.weight
Using grad with decay in Model.W_text.weight
Model.W_text.bias
Using grad with no decay in Model.W_text.bias
Model.W_av.weight
Using grad with decay in Model.W_av.weight
Model.W_av.bias
Using grad with no decay in Model.W_av.bias
Model.LN.weight
Using grad with decay in Model.LN.weight
Model.LN.bias
Using grad with no decay in Model.LN.bias
Will be using warmup for 5 steps
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([4.7231e-05], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-2.8560e-06], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-3.6294e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-8.2811e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-5.6764e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([4.2670e-05], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0013], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0024], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0027], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0030], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0032], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([5.3326e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0034], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0035], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0022], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0036], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0024], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0037], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0027], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0039], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0030], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0033], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0035], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0042], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0038], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0044], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0042], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0044], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0044], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0044], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0046], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0044], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0048], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0050], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0013], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0052], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0042], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0052], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0051], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0069], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0037], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0051], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0064], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0034], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0051], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0031], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0050], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0080], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0071], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0027], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0049], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0075], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0024], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0048], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0088], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0079], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0074], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0046], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0091], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0082], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0071], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0045], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0094], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0086], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0081], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0074], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0064], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0044], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0057], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0098], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0089], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0064], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0064], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0078], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0101], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0092], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0087], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0081], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0104], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0096], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0069], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0090], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0085], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0042], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0107], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0099], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0071], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0093], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0088], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([8.6963e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0111], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0102], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0096], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0072], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0091], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0072], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0114], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0105], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0075], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0100], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0095], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0064], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0039], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0116], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0108], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0103], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0075], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0098], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0075], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0037], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0076], device='cuda:0', requires_grad=True)
***************** Loading Model from checkpoints/lr_0001__k_2__nf_16-v2/msalm-sims-1991.pth
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0075], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0024], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0048], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
Deleting stored model from checkpoints/lr_0001__k_2__nf_16-v2/msalm-sims-1991.pth
Loading HF datasets
---------------------- Ongoing with TRAIN data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(1368, 400, 33)
(1368, 55, 709)
---------------------- Ongoing with VALID data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(456, 400, 33)
(456, 55, 709)
---------------------- Ongoing with TEST data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(457, 400, 33)
(457, 55, 709)
Ongoing with num_workers=2
ca list is: [5, 6, 7, 8, 9, 10, 11]
initializing SoftPerm
Ongoing with ----- sigmoid ----- gating
idx is 0
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 1
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 2
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 3
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 4
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 5
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 6
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Parsing decoder block: 0
Parsing decoder block: 1
Parsing decoder block: 2
Parsing decoder block: 3
Parsing decoder block: 4
Parsing decoder block: 5
COpying---------------------------------
Parsing decoder block: 6
COpying---------------------------------
Parsing decoder block: 7
COpying---------------------------------
Parsing decoder block: 8
COpying---------------------------------
Parsing decoder block: 9
COpying---------------------------------
Parsing decoder block: 10
COpying---------------------------------
Parsing decoder block: 11
COpying---------------------------------
Using BN_a
Using BN_v
----------------->>> Pretrained AudioVisual Encoder <<<<<----------------
Using BN_a
Using BN_v
----------------------- Loading AV encoder from /leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth
Copied param embed_positions_a._float_tensor
Copied param embed_positions_v._float_tensor
Copied param BN_a.weight
Copied param BN_a.bias
Copied param BN_a.running_mean
Copied param BN_a.running_var
Copied param BN_a.num_batches_tracked
Copied param BN_v.weight
Copied param BN_v.bias
Copied param BN_v.running_mean
Copied param BN_v.running_var
Copied param BN_v.num_batches_tracked
Copied param proj_a.weight
Copied param proj_v.weight
Copied param enc_a.layers.0.self_attn.in_proj_weight
Copied param enc_a.layers.0.self_attn.in_proj_bias
Copied param enc_a.layers.0.self_attn.out_proj.weight
Copied param enc_a.layers.0.self_attn.out_proj.bias
Copied param enc_a.layers.0.linear1.weight
Copied param enc_a.layers.0.linear1.bias
Copied param enc_a.layers.0.linear2.weight
Copied param enc_a.layers.0.linear2.bias
Copied param enc_a.layers.0.norm1.weight
Copied param enc_a.layers.0.norm1.bias
Copied param enc_a.layers.0.norm2.weight
Copied param enc_a.layers.0.norm2.bias
Copied param enc_a.layers.1.self_attn.in_proj_weight
Copied param enc_a.layers.1.self_attn.in_proj_bias
Copied param enc_a.layers.1.self_attn.out_proj.weight
Copied param enc_a.layers.1.self_attn.out_proj.bias
Copied param enc_a.layers.1.linear1.weight
Copied param enc_a.layers.1.linear1.bias
Copied param enc_a.layers.1.linear2.weight
Copied param enc_a.layers.1.linear2.bias
Copied param enc_a.layers.1.norm1.weight
Copied param enc_a.layers.1.norm1.bias
Copied param enc_a.layers.1.norm2.weight
Copied param enc_a.layers.1.norm2.bias
Copied param enc_a.layers.2.self_attn.in_proj_weight
Copied param enc_a.layers.2.self_attn.in_proj_bias
Copied param enc_a.layers.2.self_attn.out_proj.weight
Copied param enc_a.layers.2.self_attn.out_proj.bias
Copied param enc_a.layers.2.linear1.weight
Copied param enc_a.layers.2.linear1.bias
Copied param enc_a.layers.2.linear2.weight
Copied param enc_a.layers.2.linear2.bias
Copied param enc_a.layers.2.norm1.weight
Copied param enc_a.layers.2.norm1.bias
Copied param enc_a.layers.2.norm2.weight
Copied param enc_a.layers.2.norm2.bias
Copied param enc_v.layers.0.self_attn.in_proj_weight
Copied param enc_v.layers.0.self_attn.in_proj_bias
Copied param enc_v.layers.0.self_attn.out_proj.weight
Copied param enc_v.layers.0.self_attn.out_proj.bias
Copied param enc_v.layers.0.linear1.weight
Copied param enc_v.layers.0.linear1.bias
Copied param enc_v.layers.0.linear2.weight
Copied param enc_v.layers.0.linear2.bias
Copied param enc_v.layers.0.norm1.weight
Copied param enc_v.layers.0.norm1.bias
Copied param enc_v.layers.0.norm2.weight
Copied param enc_v.layers.0.norm2.bias
Copied param enc_v.layers.1.self_attn.in_proj_weight
Copied param enc_v.layers.1.self_attn.in_proj_bias
Copied param enc_v.layers.1.self_attn.out_proj.weight
Copied param enc_v.layers.1.self_attn.out_proj.bias
Copied param enc_v.layers.1.linear1.weight
Copied param enc_v.layers.1.linear1.bias
Copied param enc_v.layers.1.linear2.weight
Copied param enc_v.layers.1.linear2.bias
Copied param enc_v.layers.1.norm1.weight
Copied param enc_v.layers.1.norm1.bias
Copied param enc_v.layers.1.norm2.weight
Copied param enc_v.layers.1.norm2.bias
Copied param enc_v.layers.2.self_attn.in_proj_weight
Copied param enc_v.layers.2.self_attn.in_proj_bias
Copied param enc_v.layers.2.self_attn.out_proj.weight
Copied param enc_v.layers.2.self_attn.out_proj.bias
Copied param enc_v.layers.2.linear1.weight
Copied param enc_v.layers.2.linear1.bias
Copied param enc_v.layers.2.linear2.weight
Copied param enc_v.layers.2.linear2.bias
Copied param enc_v.layers.2.norm1.weight
Copied param enc_v.layers.2.norm1.bias
Copied param enc_v.layers.2.norm2.weight
Copied param enc_v.layers.2.norm2.bias
Copied param fusion.weight
Copied param fusion.bias
Copied param clf.weight
Copied param clf.bias
------------------ Adding LNorm ------------------------
ongoing with msalm
5.ca_layer.alpha_1
5.ca_layer.alpha_2
5.ca_layer.audio_expert.attn.W_q.weight
5.ca_layer.audio_expert.attn.W_kv.weight
5.ca_layer.audio_expert.attn.W_o.weight
5.ca_layer.visual_expert.attn.W_q.weight
5.ca_layer.visual_expert.attn.W_kv.weight
5.ca_layer.visual_expert.attn.W_o.weight
5.ca_layer.av_expert.attn.W_q.weight
5.ca_layer.av_expert.attn.W_kv.weight
5.ca_layer.av_expert.attn.W_o.weight
5.ca_layer.ln_1.weight
5.ca_layer.ln_1.bias
5.ca_layer.ln_2.weight
5.ca_layer.ln_2.bias
5.ca_layer.mlp.c_fc.weight
5.ca_layer.mlp.c_fc.bias
5.ca_layer.mlp.c_proj.weight
5.ca_layer.mlp.c_proj.bias
6.ca_layer.alpha_1
6.ca_layer.alpha_2
6.ca_layer.audio_expert.attn.W_q.weight
6.ca_layer.audio_expert.attn.W_kv.weight
6.ca_layer.audio_expert.attn.W_o.weight
6.ca_layer.visual_expert.attn.W_q.weight
6.ca_layer.visual_expert.attn.W_kv.weight
6.ca_layer.visual_expert.attn.W_o.weight
6.ca_layer.av_expert.attn.W_q.weight
6.ca_layer.av_expert.attn.W_kv.weight
6.ca_layer.av_expert.attn.W_o.weight
6.ca_layer.ln_1.weight
6.ca_layer.ln_1.bias
6.ca_layer.ln_2.weight
6.ca_layer.ln_2.bias
6.ca_layer.mlp.c_fc.weight
6.ca_layer.mlp.c_fc.bias
6.ca_layer.mlp.c_proj.weight
6.ca_layer.mlp.c_proj.bias
7.ca_layer.alpha_1
7.ca_layer.alpha_2
7.ca_layer.audio_expert.attn.W_q.weight
7.ca_layer.audio_expert.attn.W_kv.weight
7.ca_layer.audio_expert.attn.W_o.weight
7.ca_layer.visual_expert.attn.W_q.weight
7.ca_layer.visual_expert.attn.W_kv.weight
7.ca_layer.visual_expert.attn.W_o.weight
7.ca_layer.av_expert.attn.W_q.weight
7.ca_layer.av_expert.attn.W_kv.weight
7.ca_layer.av_expert.attn.W_o.weight
7.ca_layer.ln_1.weight
7.ca_layer.ln_1.bias
7.ca_layer.ln_2.weight
7.ca_layer.ln_2.bias
7.ca_layer.mlp.c_fc.weight
7.ca_layer.mlp.c_fc.bias
7.ca_layer.mlp.c_proj.weight
7.ca_layer.mlp.c_proj.bias
8.ca_layer.alpha_1
8.ca_layer.alpha_2
8.ca_layer.audio_expert.attn.W_q.weight
8.ca_layer.audio_expert.attn.W_kv.weight
8.ca_layer.audio_expert.attn.W_o.weight
8.ca_layer.visual_expert.attn.W_q.weight
8.ca_layer.visual_expert.attn.W_kv.weight
8.ca_layer.visual_expert.attn.W_o.weight
8.ca_layer.av_expert.attn.W_q.weight
8.ca_layer.av_expert.attn.W_kv.weight
8.ca_layer.av_expert.attn.W_o.weight
8.ca_layer.ln_1.weight
8.ca_layer.ln_1.bias
8.ca_layer.ln_2.weight
8.ca_layer.ln_2.bias
8.ca_layer.mlp.c_fc.weight
8.ca_layer.mlp.c_fc.bias
8.ca_layer.mlp.c_proj.weight
8.ca_layer.mlp.c_proj.bias
9.ca_layer.alpha_1
9.ca_layer.alpha_2
9.ca_layer.audio_expert.attn.W_q.weight
9.ca_layer.audio_expert.attn.W_kv.weight
9.ca_layer.audio_expert.attn.W_o.weight
9.ca_layer.visual_expert.attn.W_q.weight
9.ca_layer.visual_expert.attn.W_kv.weight
9.ca_layer.visual_expert.attn.W_o.weight
9.ca_layer.av_expert.attn.W_q.weight
9.ca_layer.av_expert.attn.W_kv.weight
9.ca_layer.av_expert.attn.W_o.weight
9.ca_layer.ln_1.weight
9.ca_layer.ln_1.bias
9.ca_layer.ln_2.weight
9.ca_layer.ln_2.bias
9.ca_layer.mlp.c_fc.weight
9.ca_layer.mlp.c_fc.bias
9.ca_layer.mlp.c_proj.weight
9.ca_layer.mlp.c_proj.bias
10.ca_layer.alpha_1
10.ca_layer.alpha_2
10.ca_layer.audio_expert.attn.W_q.weight
10.ca_layer.audio_expert.attn.W_kv.weight
10.ca_layer.audio_expert.attn.W_o.weight
10.ca_layer.visual_expert.attn.W_q.weight
10.ca_layer.visual_expert.attn.W_kv.weight
10.ca_layer.visual_expert.attn.W_o.weight
10.ca_layer.av_expert.attn.W_q.weight
10.ca_layer.av_expert.attn.W_kv.weight
10.ca_layer.av_expert.attn.W_o.weight
10.ca_layer.ln_1.weight
10.ca_layer.ln_1.bias
10.ca_layer.ln_2.weight
10.ca_layer.ln_2.bias
10.ca_layer.mlp.c_fc.weight
10.ca_layer.mlp.c_fc.bias
10.ca_layer.mlp.c_proj.weight
10.ca_layer.mlp.c_proj.bias
11.ca_layer.alpha_1
11.ca_layer.alpha_2
11.ca_layer.audio_expert.attn.W_q.weight
11.ca_layer.audio_expert.attn.W_kv.weight
11.ca_layer.audio_expert.attn.W_o.weight
11.ca_layer.visual_expert.attn.W_q.weight
11.ca_layer.visual_expert.attn.W_kv.weight
11.ca_layer.visual_expert.attn.W_o.weight
11.ca_layer.av_expert.attn.W_q.weight
11.ca_layer.av_expert.attn.W_kv.weight
11.ca_layer.av_expert.attn.W_o.weight
11.ca_layer.ln_1.weight
11.ca_layer.ln_1.bias
11.ca_layer.ln_2.weight
11.ca_layer.ln_2.bias
11.ca_layer.mlp.c_fc.weight
11.ca_layer.mlp.c_fc.bias
11.ca_layer.mlp.c_proj.weight
11.ca_layer.mlp.c_proj.bias
0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Model.W_task.0.weight
Model.W_task.0.bias
Model.W_task.1.weight
Model.W_task.1.bias
Model.W_task.3.weight
Model.W_task.3.bias
Model.W_bn.weight
Model.W_bn.bias
Model.W_text.weight
Model.W_text.bias
Model.W_av.weight
Model.W_av.bias
Model.av_encoder.BN_a.weight
Model.av_encoder.BN_a.bias
Model.av_encoder.BN_v.weight
Model.av_encoder.BN_v.bias
Model.av_encoder.proj_a.weight
Model.av_encoder.proj_v.weight
Model.av_encoder.enc_a.layers.0.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.0.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.0.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.0.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.0.linear1.weight
Model.av_encoder.enc_a.layers.0.linear1.bias
Model.av_encoder.enc_a.layers.0.linear2.weight
Model.av_encoder.enc_a.layers.0.linear2.bias
Model.av_encoder.enc_a.layers.0.norm1.weight
Model.av_encoder.enc_a.layers.0.norm1.bias
Model.av_encoder.enc_a.layers.0.norm2.weight
Model.av_encoder.enc_a.layers.0.norm2.bias
Model.av_encoder.enc_a.layers.1.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.1.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.1.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.1.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.1.linear1.weight
Model.av_encoder.enc_a.layers.1.linear1.bias
Model.av_encoder.enc_a.layers.1.linear2.weight
Model.av_encoder.enc_a.layers.1.linear2.bias
Model.av_encoder.enc_a.layers.1.norm1.weight
Model.av_encoder.enc_a.layers.1.norm1.bias
Model.av_encoder.enc_a.layers.1.norm2.weight
Model.av_encoder.enc_a.layers.1.norm2.bias
Model.av_encoder.enc_a.layers.2.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.2.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.2.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.2.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.2.linear1.weight
Model.av_encoder.enc_a.layers.2.linear1.bias
Model.av_encoder.enc_a.layers.2.linear2.weight
Model.av_encoder.enc_a.layers.2.linear2.bias
Model.av_encoder.enc_a.layers.2.norm1.weight
Model.av_encoder.enc_a.layers.2.norm1.bias
Model.av_encoder.enc_a.layers.2.norm2.weight
Model.av_encoder.enc_a.layers.2.norm2.bias
Model.av_encoder.enc_v.layers.0.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.0.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.0.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.0.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.0.linear1.weight
Model.av_encoder.enc_v.layers.0.linear1.bias
Model.av_encoder.enc_v.layers.0.linear2.weight
Model.av_encoder.enc_v.layers.0.linear2.bias
Model.av_encoder.enc_v.layers.0.norm1.weight
Model.av_encoder.enc_v.layers.0.norm1.bias
Model.av_encoder.enc_v.layers.0.norm2.weight
Model.av_encoder.enc_v.layers.0.norm2.bias
Model.av_encoder.enc_v.layers.1.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.1.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.1.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.1.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.1.linear1.weight
Model.av_encoder.enc_v.layers.1.linear1.bias
Model.av_encoder.enc_v.layers.1.linear2.weight
Model.av_encoder.enc_v.layers.1.linear2.bias
Model.av_encoder.enc_v.layers.1.norm1.weight
Model.av_encoder.enc_v.layers.1.norm1.bias
Model.av_encoder.enc_v.layers.1.norm2.weight
Model.av_encoder.enc_v.layers.1.norm2.bias
Model.av_encoder.enc_v.layers.2.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.2.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.2.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.2.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.2.linear1.weight
Model.av_encoder.enc_v.layers.2.linear1.bias
Model.av_encoder.enc_v.layers.2.linear2.weight
Model.av_encoder.enc_v.layers.2.linear2.bias
Model.av_encoder.enc_v.layers.2.norm1.weight
Model.av_encoder.enc_v.layers.2.norm1.bias
Model.av_encoder.enc_v.layers.2.norm2.weight
Model.av_encoder.enc_v.layers.2.norm2.bias
Model.av_encoder.fusion.weight
Model.av_encoder.fusion.bias
Model.av_encoder.clf.weight
Model.av_encoder.clf.bias
Model.LN.weight
Model.LN.bias
The total number of trainable parameters is 59.03 M
Model.lang_encoder.transformer.wte.0.embedding.weight
Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Using grad with decay in Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.wpe.0.positional.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.5.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.6.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.7.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.8.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.9.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.10.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.11.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.ln_f.weight
Model.lang_encoder.transformer.ln_f.bias
Model.W_task.0.weight
Using grad with decay in Model.W_task.0.weight
Model.W_task.0.bias
Using grad with no decay in Model.W_task.0.bias
Model.W_task.1.weight
Using grad with decay in Model.W_task.1.weight
Model.W_task.1.bias
Using grad with no decay in Model.W_task.1.bias
Model.W_task.3.weight
Using grad with decay in Model.W_task.3.weight
Model.W_task.3.bias
Using grad with no decay in Model.W_task.3.bias
Model.W_bn.weight
Using grad with decay in Model.W_bn.weight
Model.W_bn.bias
Using grad with no decay in Model.W_bn.bias
Model.W_text.weight
Using grad with decay in Model.W_text.weight
Model.W_text.bias
Using grad with no decay in Model.W_text.bias
Model.W_av.weight
Using grad with decay in Model.W_av.weight
Model.W_av.bias
Using grad with no decay in Model.W_av.bias
Model.LN.weight
Using grad with decay in Model.LN.weight
Model.LN.bias
Using grad with no decay in Model.LN.bias
Will be using warmup for 5 steps
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([6.7048e-05], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([8.5439e-05], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-8.6800e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([1.6576e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([9.0792e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-4.5408e-05], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-5.8920e-05], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-3.7187e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-4.6702e-07], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-2.5986e-06], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([4.4295e-05], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-2.2430e-05], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-9.2697e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([9.7998e-05], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([3.3433e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-3.2802e-05], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([9.2467e-05], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([1.2586e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0013], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0024], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([6.3407e-05], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0013], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-7.6295e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0027], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0030], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-8.3226e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0033], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([7.5573e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0035], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-5.1265e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0036], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0038], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([1.8934e-05], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0039], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([4.5849e-05], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0023], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([8.8168e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0042], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0025], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0027], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0045], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0029], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0046], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0032], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0046], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0034], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-3.3271e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0036], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-6.5115e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0037], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-7.9460e-06], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([5.0051e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0046], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0042], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0045], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0044], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0044], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0045], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0039], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0046], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0057], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0037], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0046], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0034], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0046], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0031], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0074], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0028], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0078], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0025], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0076], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0082], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0074], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0045], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0080], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0086], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0078], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0083], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0089], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0081], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0042], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0086], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0093], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0085], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0089], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0097], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0088], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0038], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0071], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0092], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0100], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0091], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0035], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0074], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0095], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0103], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0094], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-7.7515e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0033], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0097], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0107], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0072], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0097], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0031], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
***************** Loading Model from checkpoints/lr_0001__k_2__nf_16-v2/msalm-sims-1992.pth
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0031], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
Deleting stored model from checkpoints/lr_0001__k_2__nf_16-v2/msalm-sims-1992.pth
Loading HF datasets
---------------------- Ongoing with TRAIN data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(1368, 400, 33)
(1368, 55, 709)
---------------------- Ongoing with VALID data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(456, 400, 33)
(456, 55, 709)
---------------------- Ongoing with TEST data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(457, 400, 33)
(457, 55, 709)
Ongoing with num_workers=2
ca list is: [5, 6, 7, 8, 9, 10, 11]
initializing SoftPerm
Ongoing with ----- sigmoid ----- gating
idx is 0
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 1
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 2
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 3
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 4
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 5
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 6
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Parsing decoder block: 0
Parsing decoder block: 1
Parsing decoder block: 2
Parsing decoder block: 3
Parsing decoder block: 4
Parsing decoder block: 5
COpying---------------------------------
Parsing decoder block: 6
COpying---------------------------------
Parsing decoder block: 7
COpying---------------------------------
Parsing decoder block: 8
COpying---------------------------------
Parsing decoder block: 9
COpying---------------------------------
Parsing decoder block: 10
COpying---------------------------------
Parsing decoder block: 11
COpying---------------------------------
Using BN_a
Using BN_v
----------------->>> Pretrained AudioVisual Encoder <<<<<----------------
Using BN_a
Using BN_v
----------------------- Loading AV encoder from /leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth
Copied param embed_positions_a._float_tensor
Copied param embed_positions_v._float_tensor
Copied param BN_a.weight
Copied param BN_a.bias
Copied param BN_a.running_mean
Copied param BN_a.running_var
Copied param BN_a.num_batches_tracked
Copied param BN_v.weight
Copied param BN_v.bias
Copied param BN_v.running_mean
Copied param BN_v.running_var
Copied param BN_v.num_batches_tracked
Copied param proj_a.weight
Copied param proj_v.weight
Copied param enc_a.layers.0.self_attn.in_proj_weight
Copied param enc_a.layers.0.self_attn.in_proj_bias
Copied param enc_a.layers.0.self_attn.out_proj.weight
Copied param enc_a.layers.0.self_attn.out_proj.bias
Copied param enc_a.layers.0.linear1.weight
Copied param enc_a.layers.0.linear1.bias
Copied param enc_a.layers.0.linear2.weight
Copied param enc_a.layers.0.linear2.bias
Copied param enc_a.layers.0.norm1.weight
Copied param enc_a.layers.0.norm1.bias
Copied param enc_a.layers.0.norm2.weight
Copied param enc_a.layers.0.norm2.bias
Copied param enc_a.layers.1.self_attn.in_proj_weight
Copied param enc_a.layers.1.self_attn.in_proj_bias
Copied param enc_a.layers.1.self_attn.out_proj.weight
Copied param enc_a.layers.1.self_attn.out_proj.bias
Copied param enc_a.layers.1.linear1.weight
Copied param enc_a.layers.1.linear1.bias
Copied param enc_a.layers.1.linear2.weight
Copied param enc_a.layers.1.linear2.bias
Copied param enc_a.layers.1.norm1.weight
Copied param enc_a.layers.1.norm1.bias
Copied param enc_a.layers.1.norm2.weight
Copied param enc_a.layers.1.norm2.bias
Copied param enc_a.layers.2.self_attn.in_proj_weight
Copied param enc_a.layers.2.self_attn.in_proj_bias
Copied param enc_a.layers.2.self_attn.out_proj.weight
Copied param enc_a.layers.2.self_attn.out_proj.bias
Copied param enc_a.layers.2.linear1.weight
Copied param enc_a.layers.2.linear1.bias
Copied param enc_a.layers.2.linear2.weight
Copied param enc_a.layers.2.linear2.bias
Copied param enc_a.layers.2.norm1.weight
Copied param enc_a.layers.2.norm1.bias
Copied param enc_a.layers.2.norm2.weight
Copied param enc_a.layers.2.norm2.bias
Copied param enc_v.layers.0.self_attn.in_proj_weight
Copied param enc_v.layers.0.self_attn.in_proj_bias
Copied param enc_v.layers.0.self_attn.out_proj.weight
Copied param enc_v.layers.0.self_attn.out_proj.bias
Copied param enc_v.layers.0.linear1.weight
Copied param enc_v.layers.0.linear1.bias
Copied param enc_v.layers.0.linear2.weight
Copied param enc_v.layers.0.linear2.bias
Copied param enc_v.layers.0.norm1.weight
Copied param enc_v.layers.0.norm1.bias
Copied param enc_v.layers.0.norm2.weight
Copied param enc_v.layers.0.norm2.bias
Copied param enc_v.layers.1.self_attn.in_proj_weight
Copied param enc_v.layers.1.self_attn.in_proj_bias
Copied param enc_v.layers.1.self_attn.out_proj.weight
Copied param enc_v.layers.1.self_attn.out_proj.bias
Copied param enc_v.layers.1.linear1.weight
Copied param enc_v.layers.1.linear1.bias
Copied param enc_v.layers.1.linear2.weight
Copied param enc_v.layers.1.linear2.bias
Copied param enc_v.layers.1.norm1.weight
Copied param enc_v.layers.1.norm1.bias
Copied param enc_v.layers.1.norm2.weight
Copied param enc_v.layers.1.norm2.bias
Copied param enc_v.layers.2.self_attn.in_proj_weight
Copied param enc_v.layers.2.self_attn.in_proj_bias
Copied param enc_v.layers.2.self_attn.out_proj.weight
Copied param enc_v.layers.2.self_attn.out_proj.bias
Copied param enc_v.layers.2.linear1.weight
Copied param enc_v.layers.2.linear1.bias
Copied param enc_v.layers.2.linear2.weight
Copied param enc_v.layers.2.linear2.bias
Copied param enc_v.layers.2.norm1.weight
Copied param enc_v.layers.2.norm1.bias
Copied param enc_v.layers.2.norm2.weight
Copied param enc_v.layers.2.norm2.bias
Copied param fusion.weight
Copied param fusion.bias
Copied param clf.weight
Copied param clf.bias
------------------ Adding LNorm ------------------------
ongoing with msalm
5.ca_layer.alpha_1
5.ca_layer.alpha_2
5.ca_layer.audio_expert.attn.W_q.weight
5.ca_layer.audio_expert.attn.W_kv.weight
5.ca_layer.audio_expert.attn.W_o.weight
5.ca_layer.visual_expert.attn.W_q.weight
5.ca_layer.visual_expert.attn.W_kv.weight
5.ca_layer.visual_expert.attn.W_o.weight
5.ca_layer.av_expert.attn.W_q.weight
5.ca_layer.av_expert.attn.W_kv.weight
5.ca_layer.av_expert.attn.W_o.weight
5.ca_layer.ln_1.weight
5.ca_layer.ln_1.bias
5.ca_layer.ln_2.weight
5.ca_layer.ln_2.bias
5.ca_layer.mlp.c_fc.weight
5.ca_layer.mlp.c_fc.bias
5.ca_layer.mlp.c_proj.weight
5.ca_layer.mlp.c_proj.bias
6.ca_layer.alpha_1
6.ca_layer.alpha_2
6.ca_layer.audio_expert.attn.W_q.weight
6.ca_layer.audio_expert.attn.W_kv.weight
6.ca_layer.audio_expert.attn.W_o.weight
6.ca_layer.visual_expert.attn.W_q.weight
6.ca_layer.visual_expert.attn.W_kv.weight
6.ca_layer.visual_expert.attn.W_o.weight
6.ca_layer.av_expert.attn.W_q.weight
6.ca_layer.av_expert.attn.W_kv.weight
6.ca_layer.av_expert.attn.W_o.weight
6.ca_layer.ln_1.weight
6.ca_layer.ln_1.bias
6.ca_layer.ln_2.weight
6.ca_layer.ln_2.bias
6.ca_layer.mlp.c_fc.weight
6.ca_layer.mlp.c_fc.bias
6.ca_layer.mlp.c_proj.weight
6.ca_layer.mlp.c_proj.bias
7.ca_layer.alpha_1
7.ca_layer.alpha_2
7.ca_layer.audio_expert.attn.W_q.weight
7.ca_layer.audio_expert.attn.W_kv.weight
7.ca_layer.audio_expert.attn.W_o.weight
7.ca_layer.visual_expert.attn.W_q.weight
7.ca_layer.visual_expert.attn.W_kv.weight
7.ca_layer.visual_expert.attn.W_o.weight
7.ca_layer.av_expert.attn.W_q.weight
7.ca_layer.av_expert.attn.W_kv.weight
7.ca_layer.av_expert.attn.W_o.weight
7.ca_layer.ln_1.weight
7.ca_layer.ln_1.bias
7.ca_layer.ln_2.weight
7.ca_layer.ln_2.bias
7.ca_layer.mlp.c_fc.weight
7.ca_layer.mlp.c_fc.bias
7.ca_layer.mlp.c_proj.weight
7.ca_layer.mlp.c_proj.bias
8.ca_layer.alpha_1
8.ca_layer.alpha_2
8.ca_layer.audio_expert.attn.W_q.weight
8.ca_layer.audio_expert.attn.W_kv.weight
8.ca_layer.audio_expert.attn.W_o.weight
8.ca_layer.visual_expert.attn.W_q.weight
8.ca_layer.visual_expert.attn.W_kv.weight
8.ca_layer.visual_expert.attn.W_o.weight
8.ca_layer.av_expert.attn.W_q.weight
8.ca_layer.av_expert.attn.W_kv.weight
8.ca_layer.av_expert.attn.W_o.weight
8.ca_layer.ln_1.weight
8.ca_layer.ln_1.bias
8.ca_layer.ln_2.weight
8.ca_layer.ln_2.bias
8.ca_layer.mlp.c_fc.weight
8.ca_layer.mlp.c_fc.bias
8.ca_layer.mlp.c_proj.weight
8.ca_layer.mlp.c_proj.bias
9.ca_layer.alpha_1
9.ca_layer.alpha_2
9.ca_layer.audio_expert.attn.W_q.weight
9.ca_layer.audio_expert.attn.W_kv.weight
9.ca_layer.audio_expert.attn.W_o.weight
9.ca_layer.visual_expert.attn.W_q.weight
9.ca_layer.visual_expert.attn.W_kv.weight
9.ca_layer.visual_expert.attn.W_o.weight
9.ca_layer.av_expert.attn.W_q.weight
9.ca_layer.av_expert.attn.W_kv.weight
9.ca_layer.av_expert.attn.W_o.weight
9.ca_layer.ln_1.weight
9.ca_layer.ln_1.bias
9.ca_layer.ln_2.weight
9.ca_layer.ln_2.bias
9.ca_layer.mlp.c_fc.weight
9.ca_layer.mlp.c_fc.bias
9.ca_layer.mlp.c_proj.weight
9.ca_layer.mlp.c_proj.bias
10.ca_layer.alpha_1
10.ca_layer.alpha_2
10.ca_layer.audio_expert.attn.W_q.weight
10.ca_layer.audio_expert.attn.W_kv.weight
10.ca_layer.audio_expert.attn.W_o.weight
10.ca_layer.visual_expert.attn.W_q.weight
10.ca_layer.visual_expert.attn.W_kv.weight
10.ca_layer.visual_expert.attn.W_o.weight
10.ca_layer.av_expert.attn.W_q.weight
10.ca_layer.av_expert.attn.W_kv.weight
10.ca_layer.av_expert.attn.W_o.weight
10.ca_layer.ln_1.weight
10.ca_layer.ln_1.bias
10.ca_layer.ln_2.weight
10.ca_layer.ln_2.bias
10.ca_layer.mlp.c_fc.weight
10.ca_layer.mlp.c_fc.bias
10.ca_layer.mlp.c_proj.weight
10.ca_layer.mlp.c_proj.bias
11.ca_layer.alpha_1
11.ca_layer.alpha_2
11.ca_layer.audio_expert.attn.W_q.weight
11.ca_layer.audio_expert.attn.W_kv.weight
11.ca_layer.audio_expert.attn.W_o.weight
11.ca_layer.visual_expert.attn.W_q.weight
11.ca_layer.visual_expert.attn.W_kv.weight
11.ca_layer.visual_expert.attn.W_o.weight
11.ca_layer.av_expert.attn.W_q.weight
11.ca_layer.av_expert.attn.W_kv.weight
11.ca_layer.av_expert.attn.W_o.weight
11.ca_layer.ln_1.weight
11.ca_layer.ln_1.bias
11.ca_layer.ln_2.weight
11.ca_layer.ln_2.bias
11.ca_layer.mlp.c_fc.weight
11.ca_layer.mlp.c_fc.bias
11.ca_layer.mlp.c_proj.weight
11.ca_layer.mlp.c_proj.bias
0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Model.W_task.0.weight
Model.W_task.0.bias
Model.W_task.1.weight
Model.W_task.1.bias
Model.W_task.3.weight
Model.W_task.3.bias
Model.W_bn.weight
Model.W_bn.bias
Model.W_text.weight
Model.W_text.bias
Model.W_av.weight
Model.W_av.bias
Model.av_encoder.BN_a.weight
Model.av_encoder.BN_a.bias
Model.av_encoder.BN_v.weight
Model.av_encoder.BN_v.bias
Model.av_encoder.proj_a.weight
Model.av_encoder.proj_v.weight
Model.av_encoder.enc_a.layers.0.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.0.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.0.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.0.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.0.linear1.weight
Model.av_encoder.enc_a.layers.0.linear1.bias
Model.av_encoder.enc_a.layers.0.linear2.weight
Model.av_encoder.enc_a.layers.0.linear2.bias
Model.av_encoder.enc_a.layers.0.norm1.weight
Model.av_encoder.enc_a.layers.0.norm1.bias
Model.av_encoder.enc_a.layers.0.norm2.weight
Model.av_encoder.enc_a.layers.0.norm2.bias
Model.av_encoder.enc_a.layers.1.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.1.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.1.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.1.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.1.linear1.weight
Model.av_encoder.enc_a.layers.1.linear1.bias
Model.av_encoder.enc_a.layers.1.linear2.weight
Model.av_encoder.enc_a.layers.1.linear2.bias
Model.av_encoder.enc_a.layers.1.norm1.weight
Model.av_encoder.enc_a.layers.1.norm1.bias
Model.av_encoder.enc_a.layers.1.norm2.weight
Model.av_encoder.enc_a.layers.1.norm2.bias
Model.av_encoder.enc_a.layers.2.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.2.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.2.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.2.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.2.linear1.weight
Model.av_encoder.enc_a.layers.2.linear1.bias
Model.av_encoder.enc_a.layers.2.linear2.weight
Model.av_encoder.enc_a.layers.2.linear2.bias
Model.av_encoder.enc_a.layers.2.norm1.weight
Model.av_encoder.enc_a.layers.2.norm1.bias
Model.av_encoder.enc_a.layers.2.norm2.weight
Model.av_encoder.enc_a.layers.2.norm2.bias
Model.av_encoder.enc_v.layers.0.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.0.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.0.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.0.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.0.linear1.weight
Model.av_encoder.enc_v.layers.0.linear1.bias
Model.av_encoder.enc_v.layers.0.linear2.weight
Model.av_encoder.enc_v.layers.0.linear2.bias
Model.av_encoder.enc_v.layers.0.norm1.weight
Model.av_encoder.enc_v.layers.0.norm1.bias
Model.av_encoder.enc_v.layers.0.norm2.weight
Model.av_encoder.enc_v.layers.0.norm2.bias
Model.av_encoder.enc_v.layers.1.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.1.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.1.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.1.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.1.linear1.weight
Model.av_encoder.enc_v.layers.1.linear1.bias
Model.av_encoder.enc_v.layers.1.linear2.weight
Model.av_encoder.enc_v.layers.1.linear2.bias
Model.av_encoder.enc_v.layers.1.norm1.weight
Model.av_encoder.enc_v.layers.1.norm1.bias
Model.av_encoder.enc_v.layers.1.norm2.weight
Model.av_encoder.enc_v.layers.1.norm2.bias
Model.av_encoder.enc_v.layers.2.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.2.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.2.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.2.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.2.linear1.weight
Model.av_encoder.enc_v.layers.2.linear1.bias
Model.av_encoder.enc_v.layers.2.linear2.weight
Model.av_encoder.enc_v.layers.2.linear2.bias
Model.av_encoder.enc_v.layers.2.norm1.weight
Model.av_encoder.enc_v.layers.2.norm1.bias
Model.av_encoder.enc_v.layers.2.norm2.weight
Model.av_encoder.enc_v.layers.2.norm2.bias
Model.av_encoder.fusion.weight
Model.av_encoder.fusion.bias
Model.av_encoder.clf.weight
Model.av_encoder.clf.bias
Model.LN.weight
Model.LN.bias
The total number of trainable parameters is 59.03 M
Model.lang_encoder.transformer.wte.0.embedding.weight
Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Using grad with decay in Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.wpe.0.positional.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.5.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.6.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.7.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.8.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.9.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.10.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.11.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.ln_f.weight
Model.lang_encoder.transformer.ln_f.bias
Model.W_task.0.weight
Using grad with decay in Model.W_task.0.weight
Model.W_task.0.bias
Using grad with no decay in Model.W_task.0.bias
Model.W_task.1.weight
Using grad with decay in Model.W_task.1.weight
Model.W_task.1.bias
Using grad with no decay in Model.W_task.1.bias
Model.W_task.3.weight
Using grad with decay in Model.W_task.3.weight
Model.W_task.3.bias
Using grad with no decay in Model.W_task.3.bias
Model.W_bn.weight
Using grad with decay in Model.W_bn.weight
Model.W_bn.bias
Using grad with no decay in Model.W_bn.bias
Model.W_text.weight
Using grad with decay in Model.W_text.weight
Model.W_text.bias
Using grad with no decay in Model.W_text.bias
Model.W_av.weight
Using grad with decay in Model.W_av.weight
Model.W_av.bias
Using grad with no decay in Model.W_av.bias
Model.LN.weight
Using grad with decay in Model.LN.weight
Model.LN.bias
Using grad with no decay in Model.LN.bias
Will be using warmup for 5 steps
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([1.0097e-05], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([8.5468e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([7.8442e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([7.1459e-05], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-6.4806e-06], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([4.4177e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([6.2037e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-3.4919e-05], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0013], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-2.6317e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([8.4529e-05], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-4.7955e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0025], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0022], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-4.5324e-05], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-7.2738e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0028], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0023], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0013], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0013], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-4.9968e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0032], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0024], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-5.2400e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-4.0116e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0035], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0023], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-6.7389e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-7.6507e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0037], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0023], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-8.0376e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0039], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0022], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-9.2335e-05], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-9.8697e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-7.0223e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([-1.1077e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([1.7740e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([6.2367e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0042], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([-5.6321e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-7.8112e-05], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([5.8213e-05], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0044], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([2.6010e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0023], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0046], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0026], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0028], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0048], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0030], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0049], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0032], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0050], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0034], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0051], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0035], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0051], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0037], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0051], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0039], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0051], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0050], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0048], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0042], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0045], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0044], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0042], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0044], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0039], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0045], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0069], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0037], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0045], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0034], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-8.1821e-05], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0044], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0030], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0044], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0080], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0069], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0027], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0080], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0072], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0023], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0087], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0081], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0076], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0090], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0087], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0080], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0037], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0093], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0090], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0087], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0083], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0057], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0036], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0096], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0093], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0090], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0086], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0035], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0099], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0097], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0094], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0090], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0034], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0102], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0100], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0097], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0093], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0032], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0105], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0103], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0100], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0097], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0029], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0107], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0106], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0103], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0100], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0027], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0110], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0109], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0106], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0103], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0024], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0072], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0112], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0112], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0109], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0106], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0072], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0023], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0074], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0115], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0115], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0112], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0071], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0109], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0076], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0117], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0118], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0115], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0112], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0075], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0079], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0119], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0121], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0118], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0075], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0114], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0082], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0121], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0123], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0071], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0120], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0117], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0079], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0124], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0126], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0123], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0079], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0120], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0081], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0086], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0126], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0128], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0075], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0125], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0081], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0122], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0083], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0088], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0128], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0130], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0076], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0128], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0083], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0124], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0085], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0090], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0130], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0133], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0130], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0127], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0087], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0092], device='cuda:0', requires_grad=True)
***************** Loading Model from checkpoints/lr_0001__k_2__nf_16-v2/msalm-sims-1993.pth
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0107], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0106], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0103], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0100], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0027], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
Deleting stored model from checkpoints/lr_0001__k_2__nf_16-v2/msalm-sims-1993.pth
Loading HF datasets
---------------------- Ongoing with TRAIN data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(1368, 400, 33)
(1368, 55, 709)
---------------------- Ongoing with VALID data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(456, 400, 33)
(456, 55, 709)
---------------------- Ongoing with TEST data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(457, 400, 33)
(457, 55, 709)
Ongoing with num_workers=2
ca list is: [5, 6, 7, 8, 9, 10, 11]
initializing SoftPerm
Ongoing with ----- sigmoid ----- gating
idx is 0
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 1
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 2
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 3
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 4
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 5
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 6
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Parsing decoder block: 0
Parsing decoder block: 1
Parsing decoder block: 2
Parsing decoder block: 3
Parsing decoder block: 4
Parsing decoder block: 5
COpying---------------------------------
Parsing decoder block: 6
COpying---------------------------------
Parsing decoder block: 7
COpying---------------------------------
Parsing decoder block: 8
COpying---------------------------------
Parsing decoder block: 9
COpying---------------------------------
Parsing decoder block: 10
COpying---------------------------------
Parsing decoder block: 11
COpying---------------------------------
Using BN_a
Using BN_v
----------------->>> Pretrained AudioVisual Encoder <<<<<----------------
Using BN_a
Using BN_v
----------------------- Loading AV encoder from /leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth
Copied param embed_positions_a._float_tensor
Copied param embed_positions_v._float_tensor
Copied param BN_a.weight
Copied param BN_a.bias
Copied param BN_a.running_mean
Copied param BN_a.running_var
Copied param BN_a.num_batches_tracked
Copied param BN_v.weight
Copied param BN_v.bias
Copied param BN_v.running_mean
Copied param BN_v.running_var
Copied param BN_v.num_batches_tracked
Copied param proj_a.weight
Copied param proj_v.weight
Copied param enc_a.layers.0.self_attn.in_proj_weight
Copied param enc_a.layers.0.self_attn.in_proj_bias
Copied param enc_a.layers.0.self_attn.out_proj.weight
Copied param enc_a.layers.0.self_attn.out_proj.bias
Copied param enc_a.layers.0.linear1.weight
Copied param enc_a.layers.0.linear1.bias
Copied param enc_a.layers.0.linear2.weight
Copied param enc_a.layers.0.linear2.bias
Copied param enc_a.layers.0.norm1.weight
Copied param enc_a.layers.0.norm1.bias
Copied param enc_a.layers.0.norm2.weight
Copied param enc_a.layers.0.norm2.bias
Copied param enc_a.layers.1.self_attn.in_proj_weight
Copied param enc_a.layers.1.self_attn.in_proj_bias
Copied param enc_a.layers.1.self_attn.out_proj.weight
Copied param enc_a.layers.1.self_attn.out_proj.bias
Copied param enc_a.layers.1.linear1.weight
Copied param enc_a.layers.1.linear1.bias
Copied param enc_a.layers.1.linear2.weight
Copied param enc_a.layers.1.linear2.bias
Copied param enc_a.layers.1.norm1.weight
Copied param enc_a.layers.1.norm1.bias
Copied param enc_a.layers.1.norm2.weight
Copied param enc_a.layers.1.norm2.bias
Copied param enc_a.layers.2.self_attn.in_proj_weight
Copied param enc_a.layers.2.self_attn.in_proj_bias
Copied param enc_a.layers.2.self_attn.out_proj.weight
Copied param enc_a.layers.2.self_attn.out_proj.bias
Copied param enc_a.layers.2.linear1.weight
Copied param enc_a.layers.2.linear1.bias
Copied param enc_a.layers.2.linear2.weight
Copied param enc_a.layers.2.linear2.bias
Copied param enc_a.layers.2.norm1.weight
Copied param enc_a.layers.2.norm1.bias
Copied param enc_a.layers.2.norm2.weight
Copied param enc_a.layers.2.norm2.bias
Copied param enc_v.layers.0.self_attn.in_proj_weight
Copied param enc_v.layers.0.self_attn.in_proj_bias
Copied param enc_v.layers.0.self_attn.out_proj.weight
Copied param enc_v.layers.0.self_attn.out_proj.bias
Copied param enc_v.layers.0.linear1.weight
Copied param enc_v.layers.0.linear1.bias
Copied param enc_v.layers.0.linear2.weight
Copied param enc_v.layers.0.linear2.bias
Copied param enc_v.layers.0.norm1.weight
Copied param enc_v.layers.0.norm1.bias
Copied param enc_v.layers.0.norm2.weight
Copied param enc_v.layers.0.norm2.bias
Copied param enc_v.layers.1.self_attn.in_proj_weight
Copied param enc_v.layers.1.self_attn.in_proj_bias
Copied param enc_v.layers.1.self_attn.out_proj.weight
Copied param enc_v.layers.1.self_attn.out_proj.bias
Copied param enc_v.layers.1.linear1.weight
Copied param enc_v.layers.1.linear1.bias
Copied param enc_v.layers.1.linear2.weight
Copied param enc_v.layers.1.linear2.bias
Copied param enc_v.layers.1.norm1.weight
Copied param enc_v.layers.1.norm1.bias
Copied param enc_v.layers.1.norm2.weight
Copied param enc_v.layers.1.norm2.bias
Copied param enc_v.layers.2.self_attn.in_proj_weight
Copied param enc_v.layers.2.self_attn.in_proj_bias
Copied param enc_v.layers.2.self_attn.out_proj.weight
Copied param enc_v.layers.2.self_attn.out_proj.bias
Copied param enc_v.layers.2.linear1.weight
Copied param enc_v.layers.2.linear1.bias
Copied param enc_v.layers.2.linear2.weight
Copied param enc_v.layers.2.linear2.bias
Copied param enc_v.layers.2.norm1.weight
Copied param enc_v.layers.2.norm1.bias
Copied param enc_v.layers.2.norm2.weight
Copied param enc_v.layers.2.norm2.bias
Copied param fusion.weight
Copied param fusion.bias
Copied param clf.weight
Copied param clf.bias
------------------ Adding LNorm ------------------------
ongoing with msalm
5.ca_layer.alpha_1
5.ca_layer.alpha_2
5.ca_layer.audio_expert.attn.W_q.weight
5.ca_layer.audio_expert.attn.W_kv.weight
5.ca_layer.audio_expert.attn.W_o.weight
5.ca_layer.visual_expert.attn.W_q.weight
5.ca_layer.visual_expert.attn.W_kv.weight
5.ca_layer.visual_expert.attn.W_o.weight
5.ca_layer.av_expert.attn.W_q.weight
5.ca_layer.av_expert.attn.W_kv.weight
5.ca_layer.av_expert.attn.W_o.weight
5.ca_layer.ln_1.weight
5.ca_layer.ln_1.bias
5.ca_layer.ln_2.weight
5.ca_layer.ln_2.bias
5.ca_layer.mlp.c_fc.weight
5.ca_layer.mlp.c_fc.bias
5.ca_layer.mlp.c_proj.weight
5.ca_layer.mlp.c_proj.bias
6.ca_layer.alpha_1
6.ca_layer.alpha_2
6.ca_layer.audio_expert.attn.W_q.weight
6.ca_layer.audio_expert.attn.W_kv.weight
6.ca_layer.audio_expert.attn.W_o.weight
6.ca_layer.visual_expert.attn.W_q.weight
6.ca_layer.visual_expert.attn.W_kv.weight
6.ca_layer.visual_expert.attn.W_o.weight
6.ca_layer.av_expert.attn.W_q.weight
6.ca_layer.av_expert.attn.W_kv.weight
6.ca_layer.av_expert.attn.W_o.weight
6.ca_layer.ln_1.weight
6.ca_layer.ln_1.bias
6.ca_layer.ln_2.weight
6.ca_layer.ln_2.bias
6.ca_layer.mlp.c_fc.weight
6.ca_layer.mlp.c_fc.bias
6.ca_layer.mlp.c_proj.weight
6.ca_layer.mlp.c_proj.bias
7.ca_layer.alpha_1
7.ca_layer.alpha_2
7.ca_layer.audio_expert.attn.W_q.weight
7.ca_layer.audio_expert.attn.W_kv.weight
7.ca_layer.audio_expert.attn.W_o.weight
7.ca_layer.visual_expert.attn.W_q.weight
7.ca_layer.visual_expert.attn.W_kv.weight
7.ca_layer.visual_expert.attn.W_o.weight
7.ca_layer.av_expert.attn.W_q.weight
7.ca_layer.av_expert.attn.W_kv.weight
7.ca_layer.av_expert.attn.W_o.weight
7.ca_layer.ln_1.weight
7.ca_layer.ln_1.bias
7.ca_layer.ln_2.weight
7.ca_layer.ln_2.bias
7.ca_layer.mlp.c_fc.weight
7.ca_layer.mlp.c_fc.bias
7.ca_layer.mlp.c_proj.weight
7.ca_layer.mlp.c_proj.bias
8.ca_layer.alpha_1
8.ca_layer.alpha_2
8.ca_layer.audio_expert.attn.W_q.weight
8.ca_layer.audio_expert.attn.W_kv.weight
8.ca_layer.audio_expert.attn.W_o.weight
8.ca_layer.visual_expert.attn.W_q.weight
8.ca_layer.visual_expert.attn.W_kv.weight
8.ca_layer.visual_expert.attn.W_o.weight
8.ca_layer.av_expert.attn.W_q.weight
8.ca_layer.av_expert.attn.W_kv.weight
8.ca_layer.av_expert.attn.W_o.weight
8.ca_layer.ln_1.weight
8.ca_layer.ln_1.bias
8.ca_layer.ln_2.weight
8.ca_layer.ln_2.bias
8.ca_layer.mlp.c_fc.weight
8.ca_layer.mlp.c_fc.bias
8.ca_layer.mlp.c_proj.weight
8.ca_layer.mlp.c_proj.bias
9.ca_layer.alpha_1
9.ca_layer.alpha_2
9.ca_layer.audio_expert.attn.W_q.weight
9.ca_layer.audio_expert.attn.W_kv.weight
9.ca_layer.audio_expert.attn.W_o.weight
9.ca_layer.visual_expert.attn.W_q.weight
9.ca_layer.visual_expert.attn.W_kv.weight
9.ca_layer.visual_expert.attn.W_o.weight
9.ca_layer.av_expert.attn.W_q.weight
9.ca_layer.av_expert.attn.W_kv.weight
9.ca_layer.av_expert.attn.W_o.weight
9.ca_layer.ln_1.weight
9.ca_layer.ln_1.bias
9.ca_layer.ln_2.weight
9.ca_layer.ln_2.bias
9.ca_layer.mlp.c_fc.weight
9.ca_layer.mlp.c_fc.bias
9.ca_layer.mlp.c_proj.weight
9.ca_layer.mlp.c_proj.bias
10.ca_layer.alpha_1
10.ca_layer.alpha_2
10.ca_layer.audio_expert.attn.W_q.weight
10.ca_layer.audio_expert.attn.W_kv.weight
10.ca_layer.audio_expert.attn.W_o.weight
10.ca_layer.visual_expert.attn.W_q.weight
10.ca_layer.visual_expert.attn.W_kv.weight
10.ca_layer.visual_expert.attn.W_o.weight
10.ca_layer.av_expert.attn.W_q.weight
10.ca_layer.av_expert.attn.W_kv.weight
10.ca_layer.av_expert.attn.W_o.weight
10.ca_layer.ln_1.weight
10.ca_layer.ln_1.bias
10.ca_layer.ln_2.weight
10.ca_layer.ln_2.bias
10.ca_layer.mlp.c_fc.weight
10.ca_layer.mlp.c_fc.bias
10.ca_layer.mlp.c_proj.weight
10.ca_layer.mlp.c_proj.bias
11.ca_layer.alpha_1
11.ca_layer.alpha_2
11.ca_layer.audio_expert.attn.W_q.weight
11.ca_layer.audio_expert.attn.W_kv.weight
11.ca_layer.audio_expert.attn.W_o.weight
11.ca_layer.visual_expert.attn.W_q.weight
11.ca_layer.visual_expert.attn.W_kv.weight
11.ca_layer.visual_expert.attn.W_o.weight
11.ca_layer.av_expert.attn.W_q.weight
11.ca_layer.av_expert.attn.W_kv.weight
11.ca_layer.av_expert.attn.W_o.weight
11.ca_layer.ln_1.weight
11.ca_layer.ln_1.bias
11.ca_layer.ln_2.weight
11.ca_layer.ln_2.bias
11.ca_layer.mlp.c_fc.weight
11.ca_layer.mlp.c_fc.bias
11.ca_layer.mlp.c_proj.weight
11.ca_layer.mlp.c_proj.bias
0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Model.W_task.0.weight
Model.W_task.0.bias
Model.W_task.1.weight
Model.W_task.1.bias
Model.W_task.3.weight
Model.W_task.3.bias
Model.W_bn.weight
Model.W_bn.bias
Model.W_text.weight
Model.W_text.bias
Model.W_av.weight
Model.W_av.bias
Model.av_encoder.BN_a.weight
Model.av_encoder.BN_a.bias
Model.av_encoder.BN_v.weight
Model.av_encoder.BN_v.bias
Model.av_encoder.proj_a.weight
Model.av_encoder.proj_v.weight
Model.av_encoder.enc_a.layers.0.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.0.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.0.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.0.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.0.linear1.weight
Model.av_encoder.enc_a.layers.0.linear1.bias
Model.av_encoder.enc_a.layers.0.linear2.weight
Model.av_encoder.enc_a.layers.0.linear2.bias
Model.av_encoder.enc_a.layers.0.norm1.weight
Model.av_encoder.enc_a.layers.0.norm1.bias
Model.av_encoder.enc_a.layers.0.norm2.weight
Model.av_encoder.enc_a.layers.0.norm2.bias
Model.av_encoder.enc_a.layers.1.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.1.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.1.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.1.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.1.linear1.weight
Model.av_encoder.enc_a.layers.1.linear1.bias
Model.av_encoder.enc_a.layers.1.linear2.weight
Model.av_encoder.enc_a.layers.1.linear2.bias
Model.av_encoder.enc_a.layers.1.norm1.weight
Model.av_encoder.enc_a.layers.1.norm1.bias
Model.av_encoder.enc_a.layers.1.norm2.weight
Model.av_encoder.enc_a.layers.1.norm2.bias
Model.av_encoder.enc_a.layers.2.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.2.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.2.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.2.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.2.linear1.weight
Model.av_encoder.enc_a.layers.2.linear1.bias
Model.av_encoder.enc_a.layers.2.linear2.weight
Model.av_encoder.enc_a.layers.2.linear2.bias
Model.av_encoder.enc_a.layers.2.norm1.weight
Model.av_encoder.enc_a.layers.2.norm1.bias
Model.av_encoder.enc_a.layers.2.norm2.weight
Model.av_encoder.enc_a.layers.2.norm2.bias
Model.av_encoder.enc_v.layers.0.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.0.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.0.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.0.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.0.linear1.weight
Model.av_encoder.enc_v.layers.0.linear1.bias
Model.av_encoder.enc_v.layers.0.linear2.weight
Model.av_encoder.enc_v.layers.0.linear2.bias
Model.av_encoder.enc_v.layers.0.norm1.weight
Model.av_encoder.enc_v.layers.0.norm1.bias
Model.av_encoder.enc_v.layers.0.norm2.weight
Model.av_encoder.enc_v.layers.0.norm2.bias
Model.av_encoder.enc_v.layers.1.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.1.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.1.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.1.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.1.linear1.weight
Model.av_encoder.enc_v.layers.1.linear1.bias
Model.av_encoder.enc_v.layers.1.linear2.weight
Model.av_encoder.enc_v.layers.1.linear2.bias
Model.av_encoder.enc_v.layers.1.norm1.weight
Model.av_encoder.enc_v.layers.1.norm1.bias
Model.av_encoder.enc_v.layers.1.norm2.weight
Model.av_encoder.enc_v.layers.1.norm2.bias
Model.av_encoder.enc_v.layers.2.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.2.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.2.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.2.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.2.linear1.weight
Model.av_encoder.enc_v.layers.2.linear1.bias
Model.av_encoder.enc_v.layers.2.linear2.weight
Model.av_encoder.enc_v.layers.2.linear2.bias
Model.av_encoder.enc_v.layers.2.norm1.weight
Model.av_encoder.enc_v.layers.2.norm1.bias
Model.av_encoder.enc_v.layers.2.norm2.weight
Model.av_encoder.enc_v.layers.2.norm2.bias
Model.av_encoder.fusion.weight
Model.av_encoder.fusion.bias
Model.av_encoder.clf.weight
Model.av_encoder.clf.bias
Model.LN.weight
Model.LN.bias
The total number of trainable parameters is 59.03 M
Model.lang_encoder.transformer.wte.0.embedding.weight
Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Using grad with decay in Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.wpe.0.positional.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.5.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.6.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.7.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.8.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.9.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.10.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.11.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.ln_f.weight
Model.lang_encoder.transformer.ln_f.bias
Model.W_task.0.weight
Using grad with decay in Model.W_task.0.weight
Model.W_task.0.bias
Using grad with no decay in Model.W_task.0.bias
Model.W_task.1.weight
Using grad with decay in Model.W_task.1.weight
Model.W_task.1.bias
Using grad with no decay in Model.W_task.1.bias
Model.W_task.3.weight
Using grad with decay in Model.W_task.3.weight
Model.W_task.3.bias
Using grad with no decay in Model.W_task.3.bias
Model.W_bn.weight
Using grad with decay in Model.W_bn.weight
Model.W_bn.bias
Using grad with no decay in Model.W_bn.bias
Model.W_text.weight
Using grad with decay in Model.W_text.weight
Model.W_text.bias
Using grad with no decay in Model.W_text.bias
Model.W_av.weight
Using grad with decay in Model.W_av.weight
Model.W_av.bias
Using grad with no decay in Model.W_av.bias
Model.LN.weight
Using grad with decay in Model.LN.weight
Model.LN.bias
Using grad with no decay in Model.LN.bias
Will be using warmup for 5 steps
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-7.9303e-05], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-6.9243e-07], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([8.8448e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-3.0239e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([-4.2851e-06], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([-5.4940e-06], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0001], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0012], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([-2.1833e-05], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0015], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-9.0413e-05], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0001], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([-5.4523e-06], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0008], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0024], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([3.0471e-05], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0005], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0027], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0022], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0002], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0030], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0005], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0004], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([1.6903e-05], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([4.4627e-05], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0032], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([-2.3321e-05], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0034], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0036], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0008], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0006], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0037], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0009], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0022], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0038], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0025], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0026], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0012], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0028], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0031], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0015], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0044], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0033], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0017], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0045], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0034], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0046], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0036], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0020], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0038], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0021], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0048], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0039], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0023], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0020], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0039], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0019], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0027], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0030], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0026], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0046], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0017], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0045], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0016], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0029], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0043], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0031], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0011], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0064], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0038], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0009], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0033], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0035], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0006], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0034], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0041], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0071], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0072], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0032], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0074], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0074], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0076], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0036], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0028], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([5.4027e-05], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0037], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0078], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0080], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0025], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0004], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0038], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0082], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0081], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0021], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0040], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0085], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0085], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0088], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0018], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0011], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0040], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0039], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0089], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0089], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0092], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0014], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0014], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0041], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0038], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0057], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0092], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0092], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0096], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0057], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0081], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0010], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0018], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0043], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0037], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0095], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0096], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0100], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0035], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0064], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0099], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0099], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0103], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0088], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0003], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0071], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0025], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0034], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0062], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0102], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0103], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0064], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0107], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0091], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-5.0987e-06], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0074], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0028], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0047], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0033], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0064], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0070], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0105], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0106], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0110], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0095], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0003], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0076], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0032], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0049], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0032], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0065], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0072], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0108], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0109], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0069], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0114], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0098], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0079], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0035], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0050], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0031], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0066], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0075], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0111], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0112], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0071], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0117], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0069], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0102], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0010], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0081], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0039], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0029], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0067], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0078], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0113], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0115], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0120], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0071], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0105], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0057], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0013], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0083], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0042], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0052], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0028], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0080], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0116], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0118], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0076], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0123], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0108], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0016], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0085], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0045], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0053], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0027], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0069], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0082], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0119], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0121], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0078], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0126], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0075], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0111], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0061], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0019], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0087], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0048], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0055], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0026], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0071], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0085], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0122], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0124], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0080], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0129], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0077], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0114], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0063], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0090], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0051], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0056], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0024], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0072], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0088], device='cuda:0', requires_grad=True)
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0124], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0127], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0082], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0132], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0078], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0117], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0064], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0024], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0092], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0054], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0057], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0023], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0073], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0090], device='cuda:0', requires_grad=True)
***************** Loading Model from checkpoints/lr_0001__k_2__nf_16-v2/msalm-sims-1994.pth
Model alphas are
5.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0095], device='cuda:0', requires_grad=True)
5.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0096], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0058], device='cuda:0', requires_grad=True)
6.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0100], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0059], device='cuda:0', requires_grad=True)
7.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0084], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0046], device='cuda:0', requires_grad=True)
8.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0007], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0068], device='cuda:0', requires_grad=True)
9.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0022], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0044], device='cuda:0', requires_grad=True)
10.ca_layer.alpha_2 is: Parameter containing:
tensor([-0.0035], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_1 is: Parameter containing:
tensor([0.0060], device='cuda:0', requires_grad=True)
11.ca_layer.alpha_2 is: Parameter containing:
tensor([0.0064], device='cuda:0', requires_grad=True)
Deleting stored model from checkpoints/lr_0001__k_2__nf_16-v2/msalm-sims-1994.pth
