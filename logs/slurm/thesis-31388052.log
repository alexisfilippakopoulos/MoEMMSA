Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$Running in normal mode.$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Loading HF datasets
---------------------- Ongoing with TRAIN data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(1368, 400, 33)
(1368, 55, 709)
---------------------- Ongoing with VALID data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(456, 400, 33)
(456, 55, 709)
---------------------- Ongoing with TEST data split -----------------------------
Using GPT LM
Using Chinese LM
All the sequence lengths are L:39, A:400, V:55
Preprocessing custom M-SENA pickles
audio features are (1368, 400, 33)
vision features are (1368, 55, 709)
Starting processing --------------------->
Using chinese causal LM
(457, 400, 33)
(457, 55, 709)
Ongoing with num_workers=2
ca list is: [5, 6, 7, 8, 9, 10, 11]
initializing SoftPerm
Ongoing with ----- sigmoid ----- gating
idx is 0
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 1
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 2
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 3
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 4
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 5
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Ongoing with ----- sigmoid ----- gating
idx is 6
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
Parsing decoder block: 0
Parsing decoder block: 1
Parsing decoder block: 2
Parsing decoder block: 3
Parsing decoder block: 4
Parsing decoder block: 5
COpying---------------------------------
Parsing decoder block: 6
COpying---------------------------------
Parsing decoder block: 7
COpying---------------------------------
Parsing decoder block: 8
COpying---------------------------------
Parsing decoder block: 9
COpying---------------------------------
Parsing decoder block: 10
COpying---------------------------------
Parsing decoder block: 11
COpying---------------------------------
Using BN_a
Using BN_v
----------------->>> Pretrained AudioVisual Encoder <<<<<----------------
Using BN_a
Using BN_v
----------------------- Loading AV encoder from /leonardo_work/EUHPC_A04_051/alexfil/DeepMLF-Reprod/checkpoints/bienc-sims-1990/bienc-sims-1990.pth
Copied param embed_positions_a._float_tensor
Copied param embed_positions_v._float_tensor
Copied param BN_a.weight
Copied param BN_a.bias
Copied param BN_a.running_mean
Copied param BN_a.running_var
Copied param BN_a.num_batches_tracked
Copied param BN_v.weight
Copied param BN_v.bias
Copied param BN_v.running_mean
Copied param BN_v.running_var
Copied param BN_v.num_batches_tracked
Copied param proj_a.weight
Copied param proj_v.weight
Copied param enc_a.layers.0.self_attn.in_proj_weight
Copied param enc_a.layers.0.self_attn.in_proj_bias
Copied param enc_a.layers.0.self_attn.out_proj.weight
Copied param enc_a.layers.0.self_attn.out_proj.bias
Copied param enc_a.layers.0.linear1.weight
Copied param enc_a.layers.0.linear1.bias
Copied param enc_a.layers.0.linear2.weight
Copied param enc_a.layers.0.linear2.bias
Copied param enc_a.layers.0.norm1.weight
Copied param enc_a.layers.0.norm1.bias
Copied param enc_a.layers.0.norm2.weight
Copied param enc_a.layers.0.norm2.bias
Copied param enc_a.layers.1.self_attn.in_proj_weight
Copied param enc_a.layers.1.self_attn.in_proj_bias
Copied param enc_a.layers.1.self_attn.out_proj.weight
Copied param enc_a.layers.1.self_attn.out_proj.bias
Copied param enc_a.layers.1.linear1.weight
Copied param enc_a.layers.1.linear1.bias
Copied param enc_a.layers.1.linear2.weight
Copied param enc_a.layers.1.linear2.bias
Copied param enc_a.layers.1.norm1.weight
Copied param enc_a.layers.1.norm1.bias
Copied param enc_a.layers.1.norm2.weight
Copied param enc_a.layers.1.norm2.bias
Copied param enc_a.layers.2.self_attn.in_proj_weight
Copied param enc_a.layers.2.self_attn.in_proj_bias
Copied param enc_a.layers.2.self_attn.out_proj.weight
Copied param enc_a.layers.2.self_attn.out_proj.bias
Copied param enc_a.layers.2.linear1.weight
Copied param enc_a.layers.2.linear1.bias
Copied param enc_a.layers.2.linear2.weight
Copied param enc_a.layers.2.linear2.bias
Copied param enc_a.layers.2.norm1.weight
Copied param enc_a.layers.2.norm1.bias
Copied param enc_a.layers.2.norm2.weight
Copied param enc_a.layers.2.norm2.bias
Copied param enc_v.layers.0.self_attn.in_proj_weight
Copied param enc_v.layers.0.self_attn.in_proj_bias
Copied param enc_v.layers.0.self_attn.out_proj.weight
Copied param enc_v.layers.0.self_attn.out_proj.bias
Copied param enc_v.layers.0.linear1.weight
Copied param enc_v.layers.0.linear1.bias
Copied param enc_v.layers.0.linear2.weight
Copied param enc_v.layers.0.linear2.bias
Copied param enc_v.layers.0.norm1.weight
Copied param enc_v.layers.0.norm1.bias
Copied param enc_v.layers.0.norm2.weight
Copied param enc_v.layers.0.norm2.bias
Copied param enc_v.layers.1.self_attn.in_proj_weight
Copied param enc_v.layers.1.self_attn.in_proj_bias
Copied param enc_v.layers.1.self_attn.out_proj.weight
Copied param enc_v.layers.1.self_attn.out_proj.bias
Copied param enc_v.layers.1.linear1.weight
Copied param enc_v.layers.1.linear1.bias
Copied param enc_v.layers.1.linear2.weight
Copied param enc_v.layers.1.linear2.bias
Copied param enc_v.layers.1.norm1.weight
Copied param enc_v.layers.1.norm1.bias
Copied param enc_v.layers.1.norm2.weight
Copied param enc_v.layers.1.norm2.bias
Copied param enc_v.layers.2.self_attn.in_proj_weight
Copied param enc_v.layers.2.self_attn.in_proj_bias
Copied param enc_v.layers.2.self_attn.out_proj.weight
Copied param enc_v.layers.2.self_attn.out_proj.bias
Copied param enc_v.layers.2.linear1.weight
Copied param enc_v.layers.2.linear1.bias
Copied param enc_v.layers.2.linear2.weight
Copied param enc_v.layers.2.linear2.bias
Copied param enc_v.layers.2.norm1.weight
Copied param enc_v.layers.2.norm1.bias
Copied param enc_v.layers.2.norm2.weight
Copied param enc_v.layers.2.norm2.bias
Copied param fusion.weight
Copied param fusion.bias
Copied param clf.weight
Copied param clf.bias
------------------ Adding LNorm ------------------------
ongoing with msalm
5.ca_layer.alpha_1
5.ca_layer.alpha_2
5.ca_layer.audio_expert.attn.W_q.weight
5.ca_layer.audio_expert.attn.W_kv.weight
5.ca_layer.audio_expert.attn.W_o.weight
5.ca_layer.visual_expert.attn.W_q.weight
5.ca_layer.visual_expert.attn.W_kv.weight
5.ca_layer.visual_expert.attn.W_o.weight
5.ca_layer.av_expert.attn.W_q.weight
5.ca_layer.av_expert.attn.W_kv.weight
5.ca_layer.av_expert.attn.W_o.weight
5.ca_layer.ln_1.weight
5.ca_layer.ln_1.bias
5.ca_layer.ln_2.weight
5.ca_layer.ln_2.bias
5.ca_layer.mlp.c_fc.weight
5.ca_layer.mlp.c_fc.bias
5.ca_layer.mlp.c_proj.weight
5.ca_layer.mlp.c_proj.bias
6.ca_layer.alpha_1
6.ca_layer.alpha_2
6.ca_layer.audio_expert.attn.W_q.weight
6.ca_layer.audio_expert.attn.W_kv.weight
6.ca_layer.audio_expert.attn.W_o.weight
6.ca_layer.visual_expert.attn.W_q.weight
6.ca_layer.visual_expert.attn.W_kv.weight
6.ca_layer.visual_expert.attn.W_o.weight
6.ca_layer.av_expert.attn.W_q.weight
6.ca_layer.av_expert.attn.W_kv.weight
6.ca_layer.av_expert.attn.W_o.weight
6.ca_layer.ln_1.weight
6.ca_layer.ln_1.bias
6.ca_layer.ln_2.weight
6.ca_layer.ln_2.bias
6.ca_layer.mlp.c_fc.weight
6.ca_layer.mlp.c_fc.bias
6.ca_layer.mlp.c_proj.weight
6.ca_layer.mlp.c_proj.bias
7.ca_layer.alpha_1
7.ca_layer.alpha_2
7.ca_layer.audio_expert.attn.W_q.weight
7.ca_layer.audio_expert.attn.W_kv.weight
7.ca_layer.audio_expert.attn.W_o.weight
7.ca_layer.visual_expert.attn.W_q.weight
7.ca_layer.visual_expert.attn.W_kv.weight
7.ca_layer.visual_expert.attn.W_o.weight
7.ca_layer.av_expert.attn.W_q.weight
7.ca_layer.av_expert.attn.W_kv.weight
7.ca_layer.av_expert.attn.W_o.weight
7.ca_layer.ln_1.weight
7.ca_layer.ln_1.bias
7.ca_layer.ln_2.weight
7.ca_layer.ln_2.bias
7.ca_layer.mlp.c_fc.weight
7.ca_layer.mlp.c_fc.bias
7.ca_layer.mlp.c_proj.weight
7.ca_layer.mlp.c_proj.bias
8.ca_layer.alpha_1
8.ca_layer.alpha_2
8.ca_layer.audio_expert.attn.W_q.weight
8.ca_layer.audio_expert.attn.W_kv.weight
8.ca_layer.audio_expert.attn.W_o.weight
8.ca_layer.visual_expert.attn.W_q.weight
8.ca_layer.visual_expert.attn.W_kv.weight
8.ca_layer.visual_expert.attn.W_o.weight
8.ca_layer.av_expert.attn.W_q.weight
8.ca_layer.av_expert.attn.W_kv.weight
8.ca_layer.av_expert.attn.W_o.weight
8.ca_layer.ln_1.weight
8.ca_layer.ln_1.bias
8.ca_layer.ln_2.weight
8.ca_layer.ln_2.bias
8.ca_layer.mlp.c_fc.weight
8.ca_layer.mlp.c_fc.bias
8.ca_layer.mlp.c_proj.weight
8.ca_layer.mlp.c_proj.bias
9.ca_layer.alpha_1
9.ca_layer.alpha_2
9.ca_layer.audio_expert.attn.W_q.weight
9.ca_layer.audio_expert.attn.W_kv.weight
9.ca_layer.audio_expert.attn.W_o.weight
9.ca_layer.visual_expert.attn.W_q.weight
9.ca_layer.visual_expert.attn.W_kv.weight
9.ca_layer.visual_expert.attn.W_o.weight
9.ca_layer.av_expert.attn.W_q.weight
9.ca_layer.av_expert.attn.W_kv.weight
9.ca_layer.av_expert.attn.W_o.weight
9.ca_layer.ln_1.weight
9.ca_layer.ln_1.bias
9.ca_layer.ln_2.weight
9.ca_layer.ln_2.bias
9.ca_layer.mlp.c_fc.weight
9.ca_layer.mlp.c_fc.bias
9.ca_layer.mlp.c_proj.weight
9.ca_layer.mlp.c_proj.bias
10.ca_layer.alpha_1
10.ca_layer.alpha_2
10.ca_layer.audio_expert.attn.W_q.weight
10.ca_layer.audio_expert.attn.W_kv.weight
10.ca_layer.audio_expert.attn.W_o.weight
10.ca_layer.visual_expert.attn.W_q.weight
10.ca_layer.visual_expert.attn.W_kv.weight
10.ca_layer.visual_expert.attn.W_o.weight
10.ca_layer.av_expert.attn.W_q.weight
10.ca_layer.av_expert.attn.W_kv.weight
10.ca_layer.av_expert.attn.W_o.weight
10.ca_layer.ln_1.weight
10.ca_layer.ln_1.bias
10.ca_layer.ln_2.weight
10.ca_layer.ln_2.bias
10.ca_layer.mlp.c_fc.weight
10.ca_layer.mlp.c_fc.bias
10.ca_layer.mlp.c_proj.weight
10.ca_layer.mlp.c_proj.bias
11.ca_layer.alpha_1
11.ca_layer.alpha_2
11.ca_layer.audio_expert.attn.W_q.weight
11.ca_layer.audio_expert.attn.W_kv.weight
11.ca_layer.audio_expert.attn.W_o.weight
11.ca_layer.visual_expert.attn.W_q.weight
11.ca_layer.visual_expert.attn.W_kv.weight
11.ca_layer.visual_expert.attn.W_o.weight
11.ca_layer.av_expert.attn.W_q.weight
11.ca_layer.av_expert.attn.W_kv.weight
11.ca_layer.av_expert.attn.W_o.weight
11.ca_layer.ln_1.weight
11.ca_layer.ln_1.bias
11.ca_layer.ln_2.weight
11.ca_layer.ln_2.bias
11.ca_layer.mlp.c_fc.weight
11.ca_layer.mlp.c_fc.bias
11.ca_layer.mlp.c_proj.weight
11.ca_layer.mlp.c_proj.bias
0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Model.W_task.0.weight
Model.W_task.0.bias
Model.W_task.1.weight
Model.W_task.1.bias
Model.W_task.3.weight
Model.W_task.3.bias
Model.W_bn.weight
Model.W_bn.bias
Model.W_text.weight
Model.W_text.bias
Model.W_av.weight
Model.W_av.bias
Model.av_encoder.BN_a.weight
Model.av_encoder.BN_a.bias
Model.av_encoder.BN_v.weight
Model.av_encoder.BN_v.bias
Model.av_encoder.proj_a.weight
Model.av_encoder.proj_v.weight
Model.av_encoder.enc_a.layers.0.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.0.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.0.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.0.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.0.linear1.weight
Model.av_encoder.enc_a.layers.0.linear1.bias
Model.av_encoder.enc_a.layers.0.linear2.weight
Model.av_encoder.enc_a.layers.0.linear2.bias
Model.av_encoder.enc_a.layers.0.norm1.weight
Model.av_encoder.enc_a.layers.0.norm1.bias
Model.av_encoder.enc_a.layers.0.norm2.weight
Model.av_encoder.enc_a.layers.0.norm2.bias
Model.av_encoder.enc_a.layers.1.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.1.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.1.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.1.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.1.linear1.weight
Model.av_encoder.enc_a.layers.1.linear1.bias
Model.av_encoder.enc_a.layers.1.linear2.weight
Model.av_encoder.enc_a.layers.1.linear2.bias
Model.av_encoder.enc_a.layers.1.norm1.weight
Model.av_encoder.enc_a.layers.1.norm1.bias
Model.av_encoder.enc_a.layers.1.norm2.weight
Model.av_encoder.enc_a.layers.1.norm2.bias
Model.av_encoder.enc_a.layers.2.self_attn.in_proj_weight
Model.av_encoder.enc_a.layers.2.self_attn.in_proj_bias
Model.av_encoder.enc_a.layers.2.self_attn.out_proj.weight
Model.av_encoder.enc_a.layers.2.self_attn.out_proj.bias
Model.av_encoder.enc_a.layers.2.linear1.weight
Model.av_encoder.enc_a.layers.2.linear1.bias
Model.av_encoder.enc_a.layers.2.linear2.weight
Model.av_encoder.enc_a.layers.2.linear2.bias
Model.av_encoder.enc_a.layers.2.norm1.weight
Model.av_encoder.enc_a.layers.2.norm1.bias
Model.av_encoder.enc_a.layers.2.norm2.weight
Model.av_encoder.enc_a.layers.2.norm2.bias
Model.av_encoder.enc_v.layers.0.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.0.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.0.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.0.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.0.linear1.weight
Model.av_encoder.enc_v.layers.0.linear1.bias
Model.av_encoder.enc_v.layers.0.linear2.weight
Model.av_encoder.enc_v.layers.0.linear2.bias
Model.av_encoder.enc_v.layers.0.norm1.weight
Model.av_encoder.enc_v.layers.0.norm1.bias
Model.av_encoder.enc_v.layers.0.norm2.weight
Model.av_encoder.enc_v.layers.0.norm2.bias
Model.av_encoder.enc_v.layers.1.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.1.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.1.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.1.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.1.linear1.weight
Model.av_encoder.enc_v.layers.1.linear1.bias
Model.av_encoder.enc_v.layers.1.linear2.weight
Model.av_encoder.enc_v.layers.1.linear2.bias
Model.av_encoder.enc_v.layers.1.norm1.weight
Model.av_encoder.enc_v.layers.1.norm1.bias
Model.av_encoder.enc_v.layers.1.norm2.weight
Model.av_encoder.enc_v.layers.1.norm2.bias
Model.av_encoder.enc_v.layers.2.self_attn.in_proj_weight
Model.av_encoder.enc_v.layers.2.self_attn.in_proj_bias
Model.av_encoder.enc_v.layers.2.self_attn.out_proj.weight
Model.av_encoder.enc_v.layers.2.self_attn.out_proj.bias
Model.av_encoder.enc_v.layers.2.linear1.weight
Model.av_encoder.enc_v.layers.2.linear1.bias
Model.av_encoder.enc_v.layers.2.linear2.weight
Model.av_encoder.enc_v.layers.2.linear2.bias
Model.av_encoder.enc_v.layers.2.norm1.weight
Model.av_encoder.enc_v.layers.2.norm1.bias
Model.av_encoder.enc_v.layers.2.norm2.weight
Model.av_encoder.enc_v.layers.2.norm2.bias
Model.av_encoder.fusion.weight
Model.av_encoder.fusion.bias
Model.av_encoder.clf.weight
Model.av_encoder.clf.bias
Model.LN.weight
Model.LN.bias
The total number of trainable parameters is 59.02 M
Model.lang_encoder.transformer.wte.0.embedding.weight
Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Using grad with decay in Model.lang_encoder.transformer.wte.0.bn_embedding.bn_embedding
Model.lang_encoder.transformer.wpe.0.positional.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.0.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.0.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.0.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.1.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.1.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.1.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.1.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.2.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.2.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.2.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.2.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.3.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.3.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.3.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.3.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.4.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.4.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.4.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.4.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_1
Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.alpha_2
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.5.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.5.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.5.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.5.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.5.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.5.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.5.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_1
Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.alpha_2
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.6.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.6.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.6.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.6.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.6.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.6.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.6.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_1
Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.alpha_2
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.7.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.7.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.7.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.7.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.7.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.7.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.7.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_1
Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.alpha_2
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.8.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.8.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.8.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.8.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.8.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.8.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.8.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_1
Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.alpha_2
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.9.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.9.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.9.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.9.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.9.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.9.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.9.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_1
Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.alpha_2
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.10.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.10.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.10.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.10.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.10.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.10.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.10.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_1
Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.alpha_2
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.audio_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.visual_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_q.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_kv.weight
Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.av_expert.attn.W_o.weight
Model.lang_encoder.transformer.h.11.ca_layer.router.router_weights.weight
Model.lang_encoder.transformer.h.11.ca_layer.router.router_weights.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Using grad with decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Using grad with no decay in Model.lang_encoder.transformer.h.11.ca_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.weight
Model.lang_encoder.transformer.h.11.decoder_layer.ln_1.bias
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.weight
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_attn.bias
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.weight
Model.lang_encoder.transformer.h.11.decoder_layer.attn.c_proj.bias
Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.weight
Model.lang_encoder.transformer.h.11.decoder_layer.ln_2.bias
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.weight
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_fc.bias
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.weight
Model.lang_encoder.transformer.h.11.decoder_layer.mlp.c_proj.bias
Model.lang_encoder.transformer.ln_f.weight
Model.lang_encoder.transformer.ln_f.bias
Model.W_task.0.weight
Using grad with decay in Model.W_task.0.weight
Model.W_task.0.bias
Using grad with no decay in Model.W_task.0.bias
Model.W_task.1.weight
Using grad with decay in Model.W_task.1.weight
Model.W_task.1.bias
Using grad with no decay in Model.W_task.1.bias
Model.W_task.3.weight
Using grad with decay in Model.W_task.3.weight
Model.W_task.3.bias
Using grad with no decay in Model.W_task.3.bias
Model.W_bn.weight
Using grad with decay in Model.W_bn.weight
Model.W_bn.bias
Using grad with no decay in Model.W_bn.bias
Model.W_text.weight
Using grad with decay in Model.W_text.weight
Model.W_text.bias
Using grad with no decay in Model.W_text.bias
Model.W_av.weight
Using grad with decay in Model.W_av.weight
Model.W_av.bias
Using grad with no decay in Model.W_av.bias
Model.LN.weight
Using grad with decay in Model.LN.weight
Model.LN.bias
Using grad with no decay in Model.LN.bias
Will be using warmup for 5 steps
